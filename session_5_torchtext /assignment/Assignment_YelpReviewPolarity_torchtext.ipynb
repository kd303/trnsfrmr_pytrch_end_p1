{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_YelpReviewPolarity_torchtext.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kd303/trnsfrmr_pytrch_end_p1/blob/main/session_5_torchtext%20/assignment/Assignment_YelpReviewPolarity_torchtext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKb3KfJgzpun"
      },
      "source": [
        "from torchtext.datasets import YelpReviewPolarity"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pXFnhhMzx4R",
        "outputId": "44c57177-298e-4890-ae1e-37a343ee1be6"
      },
      "source": [
        "help(YelpReviewPolarity)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function YelpReviewPolarity in module torchtext.datasets.yelpreviewpolarity:\n",
            "\n",
            "YelpReviewPolarity(root='.data', split=('train', 'test'))\n",
            "    YelpReviewPolarity dataset\n",
            "    \n",
            "    Separately returns the train/test split\n",
            "    \n",
            "    Number of lines per split:\n",
            "        train: 560000\n",
            "    \n",
            "        test: 38000\n",
            "    \n",
            "    \n",
            "    Number of classes\n",
            "        2\n",
            "    \n",
            "    \n",
            "    Args:\n",
            "        root: Directory where the datasets are saved.\n",
            "            Default: .data\n",
            "        split: split or splits to be returned. Can be a string or tuple of strings.\n",
            "            Default: ('train', 'test')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3sWLqLmz2tp",
        "outputId": "aeda3653-7740-441e-d8b2-39d94cb727a3"
      },
      "source": [
        "train_iter = YelpReviewPolarity(split='train')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 166M/166M [00:01<00:00, 102MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouX7lqki0HbA",
        "outputId": "0cab87d7-a946-46a8-d557-a21d9290b84e"
      },
      "source": [
        "type(YelpReviewPolarity(split='train'))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchtext.data.datasets_utils._RawTextIterableDataset"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev1nEGyC0JVd",
        "outputId": "d65a21e8-057f-487d-a663-fef656f69c5c"
      },
      "source": [
        "next(train_iter)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\")"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrB4PfQA0MQi",
        "outputId": "b265fafe-b8ea-489c-e3e9-d9c5640ab443"
      },
      "source": [
        "for (line_number, (label, line)) in enumerate(train_iter):\n",
        "  print(label, line)\n",
        "  if line_number == 19:\n",
        "    break"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2 Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\n",
            "1 I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\n",
            "1 I'm writing this review to give you a heads up before you see this Doctor. The office staff and administration are very unprofessional. I left a message with multiple people regarding my bill, and no one ever called me back. I had to hound them to get an answer about my bill. \\n\\nSecond, and most important, make sure your insurance is going to cover Dr. Goldberg's visits and blood work. He recommended to me that I get a physical, and he knew I was a student because I told him. I got the physical done. Later, I found out my health insurance doesn't pay for preventative visits. I received an $800.00 bill for the blood work. I can't pay for my bill because I'm a student and don't have any cash flow at this current time. I can't believe the Doctor wouldn't give me a heads up to make sure my insurance would cover work that wasn't necessary and was strictly preventative. The office can't do anything to help me cover the bill. In addition, the office staff said the onus is on me to make sure my insurance covers visits. Frustrating situation!\n",
            "2 All the food is great here. But the best thing they have is their wings. Their wings are simply fantastic!!  The \\\"Wet Cajun\\\" are by the best & most popular.  I also like the seasoned salt wings.  Wing Night is Monday & Wednesday night, $0.75 whole wings!\\n\\nThe dining area is nice. Very family friendly! The bar is very nice is well.  This place is truly a Yinzer's dream!!  \\\"Pittsburgh Dad\\\" would love this place n'at!!\n",
            "1 Wing sauce is like water. Pretty much a lot of butter and some hot sauce (franks red hot maybe).  The whole wings are good size and crispy, but for $1 a wing the sauce could be better. The hot and extra hot are about the same flavor/heat.  The fish sandwich is good and is a large portion, sides are decent.\n",
            "1 Owning a driving range inside the city limits is like a license to print money.  I don't think I ask much out of a driving range.  Decent mats, clean balls and accessible hours.  Hell you need even less people now with the advent of the machine that doles out the balls.  This place has none of them.  It is april and there are no grass tees yet.  BTW they opened for the season this week although it has been golfing weather for a month.  The mats look like the carpet at my 107 year old aunt Irene's house.  Worn and thread bare.  Let's talk about the hours.  This place is equipped with lights yet they only sell buckets of balls until 730.  It is still light out.  Finally lets you have the pit to hit into.  When I arrived I wasn't sure if this was a driving range or an excavation site for a mastodon or a strip mining operation.  There is no grass on the range. Just mud.  Makes it a good tool to figure out how far you actually are hitting the ball.  Oh, they are cash only also.\\n\\nBottom line, this place sucks.  The best hope is that the owner sells it to someone that actually wants to make money and service golfers in Pittsburgh.\n",
            "1 This place is absolute garbage...  Half of the tees are not available, including all the grass tees.  It is cash only, and they sell the last bucket at 8, despite having lights.  And if you finish even a minute after 8, don't plan on getting a drink.  The vending machines are sold out (of course) and they sell drinks inside, but close the drawers at 8 on the dot.  There are weeds grown all over the place.  I noticed some sort of batting cage, but it looks like those are out of order as well.  Someone should buy this place and turn it into what it should be.\n",
            "2 Before I finally made it over to this range I heard the same thing from most people - it's just fine to go work on your swing. I had such a low expectation I was pleasantly surprised. \\n\\nIt's a fairly big range - if you are familiar with Scally's in Moon, it seems like it has almost as many tees, though its not nearly as nice a facility. \\n\\nThe guys in the pro shop were two of the friendlier guys I've come across at ranges or at courses. Yards were indeed marked and there are some targets to aim for, and even some hazards to aim away from. \\n\\nA big red flag to me was the extra charge ($3) to hit off the grass. I am no range expert, but this is the 4th one I've been to and the first I've seen of that sort of nickel and diming....\\n\\nPrice for the golf balls was reasonable and I do plan to be back every week until they close up in October for the season. Hopefully, since its for sale, it will reopen as a golf facility again.\n",
            "2 I drove by yesterday to get a sneak peak.  It re-opens on July 14th and I can't wait to take my kids.  The new range looks amazing.  The entire range appears to be turf, which may or many not help your game, but it looks really nice.  The tee boxes look state of the art and the club house looks like something you'll see on a newer course.  Can't wait to experience it!\n",
            "1 After waiting for almost 30 minutes to trade in an old phone part of the buy back program, our customer service rep incorrectly processed the transaction. This led to us waiting another 30 minutes for him to correct it. Don't visit this store if you want pleasant or good service.\n",
            "2 Wonderful reuben.  Map shown on Yelp page is incorrect. It is actually a different Hawkins.  I'd recommend a call for directions 412-271-9911.\n",
            "2 After a morning of Thrift Store hunting, a friend and I were thinking of lunch, and he suggested Emil's after he'd seen Chris Sebak do a bit on it and had tried it a time or two before, and I had not. He said they had a decent Reuben, but to be prepared to step back in time.\\n\\nWell, seeing as how I'm kind of addicted to late 40's and early 50's, and the whole Rat Pack scene, stepping back in time is a welcomed change in da burgh...as long as it doesn't involve 1979, which I can see all around me every day.\\n\\nAnd yet another shot at finding a decent Reuben in da burgh...well, that's like hunting the Holy Grail. So looking under one more bush certainly wouldn't hurt.\\n\\nSo off we go right at lunchtime in the middle of...where exactly were we? At first I thought we were lost, driving around a handful of very rather dismal looking blocks in what looked like a neighborhood that had been blighted by the building of a highway. And then...AHA! Here it is! And yep, there it was. This little unassuming building with an add-on entrance with what looked like a very old hand painted sign stating quite simply 'Emil's. \\n\\nWe walked in the front door, and entered another world. Another time, and another place. Oh, and any Big Burrito/Sousa foodies might as well stop reading now. I wouldn't want to see you walk in, roll your eyes and say 'Reaaaaaalllly?'\\n\\nThis is about as old world bar/lounge/restaurant as it gets. Plain, with a dark wood bar on one side, plain white walls with no yinzer pics, good sturdy chairs and actual white linens on the tables. This is the kind of neighborhood dive that I could see Frank and Dino pulling a few tables together for some poker, a fish sammich, and some cheap scotch. And THAT is exactly what I love.\\n\\nOh...but good food counts too. \\n\\nWe each had a Reuben, and my friend had a side of fries. The Reubens were decent, but not NY awesome. A little too thick on the bread, but overall, tasty and definitely filling. Not too skimpy on the meat. I seriously CRAVE a true, good NY Reuben, but since I can't afford to travel right now, what I find in da burgh will have to do. But as we sat and ate, burgers came out to an adjoining table. Those were some big thick burgers. A steak went past for the table behind us. That was HUGE! And when we asked about it, the waitress said 'Yeah, it's huge and really good, and he only charges $12.99 for it, ain't that nuts?' Another table of five came in, and wham. Fish sandwiches PILED with breaded fish that looked amazing. Yeah, I want that, that, that and THAT!\\n\\nMy friend also mentioned that they have a Chicken Parm special one day of the week that is only served UNTIL 4 pm, and that it is fantastic. If only I could GET there on that week day before 4...\\n\\nThe waitress did a good job, especially since there was quite a growing crowd at lunchtime on a Saturday, and only one of her. She kept up and was very friendly. \\n\\nThey only have Pepsi products, so I had a brewed iced tea, which was very fresh, and she did pop by to ask about refills as often as she could. As the lunch hour went on, they were getting busy.\\n\\nEmil's is no frills, good portions, very reasonable prices, VERY comfortable neighborhood hole in the wall...kind of like Cheers, but in a blue collar neighborhood in the 1950's. Fan-freakin-tastic! I could feel at home here.\\n\\nYou definitely want to hit Mapquest or plug in your GPS though. I am not sure that I could find it again on my own...it really is a hidden gem. I will be making my friend take me back until I can memorize where the heck it is.\\n\\nAddendum: 2nd visit for the fish sandwich. Excellent. Truly. A pound of fish on a fish-shaped bun (as opposed to da burgh's seemingly popular hamburger bun). The fish was flavorful, the batter excellent, and for just $8. This may have been the best fish sandwich I've yet to have in da burgh.\n",
            "2 This is a hidden gem, no really. It took us forever to find but well worth it. It is right across the street from the Rankin Police Station. The menu has a wide selection, I really couldn't decide what I wanted but I went with the ribeye sandwich. I'm glad i did too. Huge sandwich! I added mushrooms, it was very flavorful. My boyfriend got the fish sandwich, he enjoyed it as well. Fast and friendly service. Will definitely be back.\n",
            "2 Awesome drink specials during happy hour. Fantastic wings that are crispy and delicious, wing night on Tuesday and Thursday!\n",
            "1 Very disappointed in the customer service. We ordered Reuben's  and wanted coleslaw instead of kraut. They charged us $3.00 for the coleslaw. We will not be back . The iced tea is also terrible tasting.\n",
            "1 Used to go there for tires, brakes, etc.  Their prices have gone way up-$400 for 4 mid-level tires for a Toyota.  Plus, 1 of the new tires went flat within 3 weeks.  Since they don\\\"t make appointments,  the wait to get the tire looked at was ~2 hours.  Sorry--can't wait that long to get a warranted repair,  They lost my business for good.\n",
            "1 I got 'new' tires from them and within two weeks got a flat. I took my car to a local mechanic to see if i could get the hole patched, but they said the reason I had a flat was because the previous patch had blown - WAIT, WHAT? I just got the tire and never needed to have it patched? This was supposed to be a new tire. \\nI took the tire over to Flynn's and they told me that someone punctured my tire, then tried to patch it. So there are resentful tire slashers? I find that very unlikely. After arguing with the guy and telling him that his logic was far fetched he said he'd give me a new tire \\\"this time\\\". \\nI will never go back to Flynn's b/c of the way this guy treated me and the simple fact that they gave me a used tire!\n",
            "1 Terrible. Preordered my tires and when I arrived they couldn't find the order anywhere. Once we got through that process I waited over 2 hours for them to be put on... I was originally told it would take 30 mins. Slow, over priced, I'll go elsewhere next time.\n",
            "2 I've been informed by a fellow Yelper that they are just closed for the season, I hope they're right! I find it strange they don't keep their phone number in service with a recording and that they didn't cover the course up to protect it from weather, vandalism, etc. Either way, I can't wait to get my game on in the spring!\n",
            "1 Don't waste your time.  We had two different people come to our house to give us estimates for a deck (one of them the OWNER).  Both times, we never heard from them.  Not a call, not the estimate, nothing.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7LwnrU-0cMI",
        "outputId": "cd56e124-487a-4942-bc37-7ed0058875d8"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_iter = YelpReviewPolarity(split = 'train')\n",
        "\n",
        "help(DataLoader)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class DataLoader in module torch.utils.data.dataloader:\n",
            "\n",
            "class DataLoader(typing.Generic)\n",
            " |  DataLoader(*args, **kwds)\n",
            " |  \n",
            " |  Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
            " |  the given dataset.\n",
            " |  \n",
            " |  The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
            " |  iterable-style datasets with single- or multi-process loading, customizing\n",
            " |  loading order and optional automatic batching (collation) and memory pinning.\n",
            " |  \n",
            " |  See :py:mod:`torch.utils.data` documentation page for more details.\n",
            " |  \n",
            " |  Args:\n",
            " |      dataset (Dataset): dataset from which to load the data.\n",
            " |      batch_size (int, optional): how many samples per batch to load\n",
            " |          (default: ``1``).\n",
            " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
            " |          at every epoch (default: ``False``).\n",
            " |      sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
            " |          samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
            " |          implemented. If specified, :attr:`shuffle` must not be specified.\n",
            " |      batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
            " |          returns a batch of indices at a time. Mutually exclusive with\n",
            " |          :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
            " |          and :attr:`drop_last`.\n",
            " |      num_workers (int, optional): how many subprocesses to use for data\n",
            " |          loading. ``0`` means that the data will be loaded in the main process.\n",
            " |          (default: ``0``)\n",
            " |      collate_fn (callable, optional): merges a list of samples to form a\n",
            " |          mini-batch of Tensor(s).  Used when using batched loading from a\n",
            " |          map-style dataset.\n",
            " |      pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
            " |          into CUDA pinned memory before returning them.  If your data elements\n",
            " |          are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
            " |          see the example below.\n",
            " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
            " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
            " |          the size of dataset is not divisible by the batch size, then the last batch\n",
            " |          will be smaller. (default: ``False``)\n",
            " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
            " |          from workers. Should always be non-negative. (default: ``0``)\n",
            " |      worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
            " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
            " |          input, after seeding and before data loading. (default: ``None``)\n",
            " |      generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
            " |          by RandomSampler to generate random indexes and multiprocessing to generate\n",
            " |          `base_seed` for workers. (default: ``None``)\n",
            " |      prefetch_factor (int, optional, keyword-only arg): Number of samples loaded\n",
            " |          in advance by each worker. ``2`` means there will be a total of\n",
            " |          2 * num_workers samples prefetched across all workers. (default: ``2``)\n",
            " |      persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
            " |          the worker processes after a dataset has been consumed once. This allows to\n",
            " |          maintain the workers `Dataset` instances alive. (default: ``False``)\n",
            " |  \n",
            " |  \n",
            " |  .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
            " |               cannot be an unpicklable object, e.g., a lambda function. See\n",
            " |               :ref:`multiprocessing-best-practices` on more details related\n",
            " |               to multiprocessing in PyTorch.\n",
            " |  \n",
            " |  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
            " |               When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
            " |               it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
            " |               rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
            " |               configurations. This represents the best guess PyTorch can make because PyTorch\n",
            " |               trusts user :attr:`dataset` code in correctly handling multi-process\n",
            " |               loading to avoid duplicate data.\n",
            " |  \n",
            " |               However, if sharding results in multiple workers having incomplete last batches,\n",
            " |               this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
            " |               be broken into multiple ones and (2) more than one batch worth of samples can be\n",
            " |               dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
            " |               cases in general.\n",
            " |  \n",
            " |               See `Dataset Types`_ for more details on these two types of datasets and how\n",
            " |               :class:`~torch.utils.data.IterableDataset` interacts with\n",
            " |               `Multi-process data loading`_.\n",
            " |  \n",
            " |  .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
            " |               :ref:`data-loading-randomness` notes for random seed related questions.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DataLoader\n",
            " |      typing.Generic\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, dataset: torch.utils.data.dataset.Dataset[+T_co], batch_size: Union[int, NoneType] = 1, shuffle: bool = False, sampler: Union[torch.utils.data.sampler.Sampler, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[Sequence], NoneType] = None, num_workers: int = 0, collate_fn: Union[Callable[[List[~T]], Any], NoneType] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Union[Callable[[int], NoneType], NoneType] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self) -> '_BaseDataLoaderIter'\n",
            " |      # We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up\n",
            " |      # since '_BaseDataLoaderIter' references 'DataLoader'.\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |  \n",
            " |  __setattr__(self, attr, val)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  check_worker_number_rationality(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  multiprocessing_context\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'_iterator': typing.Union[ForwardRef('_BaseDataLoad...\n",
            " |  \n",
            " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
            " |  \n",
            " |  __parameters__ = (+T_co,)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __class_getitem__(params) from builtins.type\n",
            " |  \n",
            " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwds)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_vsHPV_1TZ-"
      },
      "source": [
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_4z8hD41_fI",
        "outputId": "9810622d-9895-4fec-9c0b-0b27ea3dc892"
      },
      "source": [
        "next(iter(dataloader))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([1, 2, 1, 1, 2, 1, 1, 1]),\n",
              " (\"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\",\n",
              "  \"Been going to Dr. Goldberg for over 10 years. I think I was one of his 1st patients when he started at MHMG. He's been great over the years and is really all about the big picture. It is because of him, not my now former gyn Dr. Markoff, that I found out I have fibroids. He explores all options with you and is very patient and understanding. He doesn't judge and asks all the right questions. Very thorough and wants to be kept in the loop on every aspect of your medical health and your life.\",\n",
              "  \"I don't know what Dr. Goldberg was like before  moving to Arizona, but let me tell you, STAY AWAY from this doctor and this office. I was going to Dr. Johnson before he left and Goldberg took over when Johnson left. He is not a caring doctor. He is only interested in the co-pay and having you come in for medication refills every month. He will not give refills and could less about patients's financial situations. Trying to get your 90 days mail away pharmacy prescriptions through this guy is a joke. And to make matters even worse, his office staff is incompetent. 90% of the time when you call the office, they'll put you through to a voice mail, that NO ONE ever answers or returns your call. Both my adult children and husband have decided to leave this practice after experiencing such frustration. The entire office has an attitude like they are doing you a favor. Give me a break! Stay away from this doc and the practice. You deserve better and they will not be there when you really need them. I have never felt compelled to write a bad review about anyone until I met this pathetic excuse for a doctor who is all about the money.\",\n",
              "  \"I'm writing this review to give you a heads up before you see this Doctor. The office staff and administration are very unprofessional. I left a message with multiple people regarding my bill, and no one ever called me back. I had to hound them to get an answer about my bill. \\\\n\\\\nSecond, and most important, make sure your insurance is going to cover Dr. Goldberg's visits and blood work. He recommended to me that I get a physical, and he knew I was a student because I told him. I got the physical done. Later, I found out my health insurance doesn't pay for preventative visits. I received an $800.00 bill for the blood work. I can't pay for my bill because I'm a student and don't have any cash flow at this current time. I can't believe the Doctor wouldn't give me a heads up to make sure my insurance would cover work that wasn't necessary and was strictly preventative. The office can't do anything to help me cover the bill. In addition, the office staff said the onus is on me to make sure my insurance covers visits. Frustrating situation!\",\n",
              "  'All the food is great here. But the best thing they have is their wings. Their wings are simply fantastic!!  The \\\\\"Wet Cajun\\\\\" are by the best & most popular.  I also like the seasoned salt wings.  Wing Night is Monday & Wednesday night, $0.75 whole wings!\\\\n\\\\nThe dining area is nice. Very family friendly! The bar is very nice is well.  This place is truly a Yinzer\\'s dream!!  \\\\\"Pittsburgh Dad\\\\\" would love this place n\\'at!!',\n",
              "  'Wing sauce is like water. Pretty much a lot of butter and some hot sauce (franks red hot maybe).  The whole wings are good size and crispy, but for $1 a wing the sauce could be better. The hot and extra hot are about the same flavor/heat.  The fish sandwich is good and is a large portion, sides are decent.',\n",
              "  \"Owning a driving range inside the city limits is like a license to print money.  I don't think I ask much out of a driving range.  Decent mats, clean balls and accessible hours.  Hell you need even less people now with the advent of the machine that doles out the balls.  This place has none of them.  It is april and there are no grass tees yet.  BTW they opened for the season this week although it has been golfing weather for a month.  The mats look like the carpet at my 107 year old aunt Irene's house.  Worn and thread bare.  Let's talk about the hours.  This place is equipped with lights yet they only sell buckets of balls until 730.  It is still light out.  Finally lets you have the pit to hit into.  When I arrived I wasn't sure if this was a driving range or an excavation site for a mastodon or a strip mining operation.  There is no grass on the range. Just mud.  Makes it a good tool to figure out how far you actually are hitting the ball.  Oh, they are cash only also.\\\\n\\\\nBottom line, this place sucks.  The best hope is that the owner sells it to someone that actually wants to make money and service golfers in Pittsburgh.\",\n",
              "  \"This place is absolute garbage...  Half of the tees are not available, including all the grass tees.  It is cash only, and they sell the last bucket at 8, despite having lights.  And if you finish even a minute after 8, don't plan on getting a drink.  The vending machines are sold out (of course) and they sell drinks inside, but close the drawers at 8 on the dot.  There are weeds grown all over the place.  I noticed some sort of batting cage, but it looks like those are out of order as well.  Someone should buy this place and turn it into what it should be.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq-IuEKY2GQR"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdxunJbc3FUH",
        "outputId": "07948b5b-e538-4c60-ec10-dad08f201386"
      },
      "source": [
        "help(get_tokenizer)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function get_tokenizer in module torchtext.data.utils:\n",
            "\n",
            "get_tokenizer(tokenizer, language='en')\n",
            "    Generate tokenizer function for a string sentence.\n",
            "    \n",
            "    Args:\n",
            "        tokenizer: the name of tokenizer function. If None, it returns split()\n",
            "            function, which splits the string sentence by space.\n",
            "            If basic_english, it returns _basic_english_normalize() function,\n",
            "            which normalize the string first and split by space. If a callable\n",
            "            function, it will return the function. If a tokenizer library\n",
            "            (e.g. spacy, moses, toktok, revtok, subword), it returns the\n",
            "            corresponding library.\n",
            "        language: Default en\n",
            "    \n",
            "    Examples:\n",
            "        >>> import torchtext\n",
            "        >>> from torchtext.data import get_tokenizer\n",
            "        >>> tokenizer = get_tokenizer(\"basic_english\")\n",
            "        >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
            "        >>> tokens\n",
            "        >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y2YR4UV3GOi"
      },
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oALqqil43mX2"
      },
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "train_iter = YelpReviewPolarity(split = 'train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6Cp7Qbw6xS1"
      },
      "source": [
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK1sxfsM7doh"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSeLl_Sh-6ON"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)    "
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRDpOkXiAqyo"
      },
      "source": [
        "train_iter = YelpReviewPolarity(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-IOBmF7A1N-"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjT2yjCYZej6"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW1rS0OYBGhR"
      },
      "source": [
        "train_iter = YelpReviewPolarity(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtVCypgvBQfM"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # disuccees\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV7lPedpBfpS",
        "outputId": "ef4aa6ac-e5dc-4310-c774-237df550d567"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = YelpReviewPolarity()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 8313 batches | accuracy    0.777\n",
            "| epoch   1 |  1000/ 8313 batches | accuracy    0.870\n",
            "| epoch   1 |  1500/ 8313 batches | accuracy    0.881\n",
            "| epoch   1 |  2000/ 8313 batches | accuracy    0.890\n",
            "| epoch   1 |  2500/ 8313 batches | accuracy    0.893\n",
            "| epoch   1 |  3000/ 8313 batches | accuracy    0.894\n",
            "| epoch   1 |  3500/ 8313 batches | accuracy    0.899\n",
            "| epoch   1 |  4000/ 8313 batches | accuracy    0.902\n",
            "| epoch   1 |  4500/ 8313 batches | accuracy    0.908\n",
            "| epoch   1 |  5000/ 8313 batches | accuracy    0.905\n",
            "| epoch   1 |  5500/ 8313 batches | accuracy    0.908\n",
            "| epoch   1 |  6000/ 8313 batches | accuracy    0.910\n",
            "| epoch   1 |  6500/ 8313 batches | accuracy    0.910\n",
            "| epoch   1 |  7000/ 8313 batches | accuracy    0.912\n",
            "| epoch   1 |  7500/ 8313 batches | accuracy    0.911\n",
            "| epoch   1 |  8000/ 8313 batches | accuracy    0.907\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 78.59s | valid accuracy    0.921 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  1000/ 8313 batches | accuracy    0.914\n",
            "| epoch   2 |  1500/ 8313 batches | accuracy    0.914\n",
            "| epoch   2 |  2000/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  2500/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  3000/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  3500/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  4000/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  4500/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  5000/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  5500/ 8313 batches | accuracy    0.915\n",
            "| epoch   2 |  6000/ 8313 batches | accuracy    0.921\n",
            "| epoch   2 |  6500/ 8313 batches | accuracy    0.915\n",
            "| epoch   2 |  7000/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  7500/ 8313 batches | accuracy    0.923\n",
            "| epoch   2 |  8000/ 8313 batches | accuracy    0.919\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 77.55s | valid accuracy    0.901 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 8313 batches | accuracy    0.931\n",
            "| epoch   3 |  1000/ 8313 batches | accuracy    0.931\n",
            "| epoch   3 |  1500/ 8313 batches | accuracy    0.932\n",
            "| epoch   3 |  2000/ 8313 batches | accuracy    0.932\n",
            "| epoch   3 |  2500/ 8313 batches | accuracy    0.933\n",
            "| epoch   3 |  3000/ 8313 batches | accuracy    0.933\n",
            "| epoch   3 |  3500/ 8313 batches | accuracy    0.932\n",
            "| epoch   3 |  4000/ 8313 batches | accuracy    0.932\n",
            "| epoch   3 |  4500/ 8313 batches | accuracy    0.932\n",
            "| epoch   3 |  5000/ 8313 batches | accuracy    0.933\n",
            "| epoch   3 |  5500/ 8313 batches | accuracy    0.932\n",
            "| epoch   3 |  6000/ 8313 batches | accuracy    0.932\n",
            "| epoch   3 |  6500/ 8313 batches | accuracy    0.933\n",
            "| epoch   3 |  7000/ 8313 batches | accuracy    0.935\n",
            "| epoch   3 |  7500/ 8313 batches | accuracy    0.932\n",
            "| epoch   3 |  8000/ 8313 batches | accuracy    0.934\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 78.04s | valid accuracy    0.931 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 8313 batches | accuracy    0.933\n",
            "| epoch   4 |  1000/ 8313 batches | accuracy    0.935\n",
            "| epoch   4 |  1500/ 8313 batches | accuracy    0.933\n",
            "| epoch   4 |  2000/ 8313 batches | accuracy    0.933\n",
            "| epoch   4 |  2500/ 8313 batches | accuracy    0.933\n",
            "| epoch   4 |  3000/ 8313 batches | accuracy    0.931\n",
            "| epoch   4 |  3500/ 8313 batches | accuracy    0.933\n",
            "| epoch   4 |  4000/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  4500/ 8313 batches | accuracy    0.935\n",
            "| epoch   4 |  5000/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  5500/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  6000/ 8313 batches | accuracy    0.932\n",
            "| epoch   4 |  6500/ 8313 batches | accuracy    0.933\n",
            "| epoch   4 |  7000/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  7500/ 8313 batches | accuracy    0.936\n",
            "| epoch   4 |  8000/ 8313 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 78.00s | valid accuracy    0.930 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 8313 batches | accuracy    0.933\n",
            "| epoch   5 |  1000/ 8313 batches | accuracy    0.936\n",
            "| epoch   5 |  1500/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  2000/ 8313 batches | accuracy    0.934\n",
            "| epoch   5 |  2500/ 8313 batches | accuracy    0.936\n",
            "| epoch   5 |  3000/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  3500/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  4000/ 8313 batches | accuracy    0.936\n",
            "| epoch   5 |  4500/ 8313 batches | accuracy    0.934\n",
            "| epoch   5 |  5000/ 8313 batches | accuracy    0.936\n",
            "| epoch   5 |  5500/ 8313 batches | accuracy    0.937\n",
            "| epoch   5 |  6000/ 8313 batches | accuracy    0.934\n",
            "| epoch   5 |  6500/ 8313 batches | accuracy    0.934\n",
            "| epoch   5 |  7000/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  7500/ 8313 batches | accuracy    0.933\n",
            "| epoch   5 |  8000/ 8313 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 78.19s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 8313 batches | accuracy    0.932\n",
            "| epoch   6 |  1000/ 8313 batches | accuracy    0.938\n",
            "| epoch   6 |  1500/ 8313 batches | accuracy    0.934\n",
            "| epoch   6 |  2000/ 8313 batches | accuracy    0.934\n",
            "| epoch   6 |  2500/ 8313 batches | accuracy    0.937\n",
            "| epoch   6 |  3000/ 8313 batches | accuracy    0.935\n",
            "| epoch   6 |  3500/ 8313 batches | accuracy    0.936\n",
            "| epoch   6 |  4000/ 8313 batches | accuracy    0.937\n",
            "| epoch   6 |  4500/ 8313 batches | accuracy    0.935\n",
            "| epoch   6 |  5000/ 8313 batches | accuracy    0.934\n",
            "| epoch   6 |  5500/ 8313 batches | accuracy    0.934\n",
            "| epoch   6 |  6000/ 8313 batches | accuracy    0.934\n",
            "| epoch   6 |  6500/ 8313 batches | accuracy    0.935\n",
            "| epoch   6 |  7000/ 8313 batches | accuracy    0.936\n",
            "| epoch   6 |  7500/ 8313 batches | accuracy    0.938\n",
            "| epoch   6 |  8000/ 8313 batches | accuracy    0.934\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 79.97s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 8313 batches | accuracy    0.935\n",
            "| epoch   7 |  1000/ 8313 batches | accuracy    0.934\n",
            "| epoch   7 |  1500/ 8313 batches | accuracy    0.934\n",
            "| epoch   7 |  2000/ 8313 batches | accuracy    0.937\n",
            "| epoch   7 |  2500/ 8313 batches | accuracy    0.936\n",
            "| epoch   7 |  3000/ 8313 batches | accuracy    0.933\n",
            "| epoch   7 |  3500/ 8313 batches | accuracy    0.936\n",
            "| epoch   7 |  4000/ 8313 batches | accuracy    0.936\n",
            "| epoch   7 |  4500/ 8313 batches | accuracy    0.936\n",
            "| epoch   7 |  5000/ 8313 batches | accuracy    0.934\n",
            "| epoch   7 |  5500/ 8313 batches | accuracy    0.934\n",
            "| epoch   7 |  6000/ 8313 batches | accuracy    0.936\n",
            "| epoch   7 |  6500/ 8313 batches | accuracy    0.936\n",
            "| epoch   7 |  7000/ 8313 batches | accuracy    0.934\n",
            "| epoch   7 |  7500/ 8313 batches | accuracy    0.935\n",
            "| epoch   7 |  8000/ 8313 batches | accuracy    0.936\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 79.71s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 8313 batches | accuracy    0.934\n",
            "| epoch   8 |  1000/ 8313 batches | accuracy    0.934\n",
            "| epoch   8 |  1500/ 8313 batches | accuracy    0.936\n",
            "| epoch   8 |  2000/ 8313 batches | accuracy    0.938\n",
            "| epoch   8 |  2500/ 8313 batches | accuracy    0.937\n",
            "| epoch   8 |  3000/ 8313 batches | accuracy    0.934\n",
            "| epoch   8 |  3500/ 8313 batches | accuracy    0.934\n",
            "| epoch   8 |  4000/ 8313 batches | accuracy    0.937\n",
            "| epoch   8 |  4500/ 8313 batches | accuracy    0.937\n",
            "| epoch   8 |  5000/ 8313 batches | accuracy    0.936\n",
            "| epoch   8 |  5500/ 8313 batches | accuracy    0.936\n",
            "| epoch   8 |  6000/ 8313 batches | accuracy    0.936\n",
            "| epoch   8 |  6500/ 8313 batches | accuracy    0.933\n",
            "| epoch   8 |  7000/ 8313 batches | accuracy    0.934\n",
            "| epoch   8 |  7500/ 8313 batches | accuracy    0.934\n",
            "| epoch   8 |  8000/ 8313 batches | accuracy    0.932\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 81.05s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 8313 batches | accuracy    0.934\n",
            "| epoch   9 |  1000/ 8313 batches | accuracy    0.934\n",
            "| epoch   9 |  1500/ 8313 batches | accuracy    0.935\n",
            "| epoch   9 |  2000/ 8313 batches | accuracy    0.935\n",
            "| epoch   9 |  2500/ 8313 batches | accuracy    0.935\n",
            "| epoch   9 |  3000/ 8313 batches | accuracy    0.934\n",
            "| epoch   9 |  3500/ 8313 batches | accuracy    0.937\n",
            "| epoch   9 |  4000/ 8313 batches | accuracy    0.934\n",
            "| epoch   9 |  4500/ 8313 batches | accuracy    0.937\n",
            "| epoch   9 |  5000/ 8313 batches | accuracy    0.936\n",
            "| epoch   9 |  5500/ 8313 batches | accuracy    0.937\n",
            "| epoch   9 |  6000/ 8313 batches | accuracy    0.934\n",
            "| epoch   9 |  6500/ 8313 batches | accuracy    0.936\n",
            "| epoch   9 |  7000/ 8313 batches | accuracy    0.935\n",
            "| epoch   9 |  7500/ 8313 batches | accuracy    0.936\n",
            "| epoch   9 |  8000/ 8313 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 79.81s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  1000/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  1500/ 8313 batches | accuracy    0.936\n",
            "| epoch  10 |  2000/ 8313 batches | accuracy    0.938\n",
            "| epoch  10 |  2500/ 8313 batches | accuracy    0.936\n",
            "| epoch  10 |  3000/ 8313 batches | accuracy    0.935\n",
            "| epoch  10 |  3500/ 8313 batches | accuracy    0.935\n",
            "| epoch  10 |  4000/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  4500/ 8313 batches | accuracy    0.935\n",
            "| epoch  10 |  5000/ 8313 batches | accuracy    0.937\n",
            "| epoch  10 |  5500/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  6000/ 8313 batches | accuracy    0.937\n",
            "| epoch  10 |  6500/ 8313 batches | accuracy    0.932\n",
            "| epoch  10 |  7000/ 8313 batches | accuracy    0.937\n",
            "| epoch  10 |  7500/ 8313 batches | accuracy    0.937\n",
            "| epoch  10 |  8000/ 8313 batches | accuracy    0.934\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 79.98s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qx3mEqsP0F2"
      },
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUrnPMhVBlq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f9c4c9-75ac-4c27-ac39-84a7bd798bc5"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76PGhuKbB4BN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11c8f84-06f3-491c-c1e6-0ef08171ca2f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a Business news\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-bXp1P9B55k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}