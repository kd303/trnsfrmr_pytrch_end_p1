## Session 5 - Torch Text introduction

1. Normalizing 
  - At layers, at wieghts
  - converts to normal distribution
  - Normalized amplitudes of weights at layers speeds up the training, if not done on all layers backprop will have relative updates to all weights
2. Batch Normalizing
  - covariate shift - Internal covariate shift (good, great, excellent -> you dont need them to go different distribution)
  - Layer, batch, group normalization, NLP -> Layer or Group, Image - batch
    - Layer - sum of all input vectors in a batch
    - Batch - individual feature vector
    - 
6. REgularization
   - Data Augmention
   - Wieght 
