{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kd303/trnsfrmr_pytrch_end_p1/blob/main/advanced_rnd/ReplitLM_trail.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9lywOih0pzj"
      },
      "source": [
        "# Replit Code v 3b model inferencing\n",
        "\n",
        "Thanks to original [Replit Repo](https://github.com/replit/ReplitLM/blob/main/replit-code-v1-3b/README.md) for\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oB_nc-30mgc"
      },
      "outputs": [],
      "source": [
        "# Uncomment following\n",
        "!pip install einops sentencepiece torch transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# this is for inferencing and FlashAttention, triton libraries takes lot of time to install (approx 15-18 mins)\n",
        "!pip install flash-attn==0.2.8 triton==2.0.0.dev20221202"
      ],
      "metadata": {
        "id": "0Xf_cttO2You"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('replit/replit-code-v1-3b', trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained('replit/replit-code-v1-3b', trust_remote_code=True)\n",
        "model.to(device='cuda:0', dtype=torch.bfloat16)\n",
        "x = tokenizer.encode('def cosine(n): ', return_tensors='pt')\n",
        "x = x.to(device='cuda:0')\n",
        "y = model.generate(x, max_length=100, do_sample=True, top_p=0.95, top_k=4, temperature=0.2, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# decoding, clean_up_tokenization_spaces=False to ensure syntactical correctness\n",
        "generated_code = tokenizer.decode(y[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "print(generated_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TphkeTiY21Jm",
        "outputId": "9766e04f-51d3-482f-8403-69d0e74aa160"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are using config.init_device='cpu', but you can also use config.init_device=\"meta\" with Composer + FSDP for fast initialization.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def fibonacci(n): # n is the number of terms to return\n",
            "    a, b = 0, 1\n",
            "    while a < n:\n",
            "        yield a\n",
            "        a, b = b, a + b\n",
            "\n",
            "def fibonacci_gen(): # generator function\n",
            "    a, b = 0, 1\n",
            "    while True:\n",
            "        yield a\n",
            "        a, b = \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! find . config.json"
      ],
      "metadata": {
        "id": "3rGtoy5cM9oX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tokenizer.encode('bubble_sort_with_matrix(n[]){} ', return_tensors='pt')\n",
        "x = x.to(device='cuda:0')\n",
        "y = model.generate(x, max_length=1024, do_sample=True, top_p=0.95, top_k=4, temperature=0.2, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)\n",
        "\n",
        "# decoding, clean_up_tokenization_spaces=False to ensure syntactical correctness\n",
        "generated_code = tokenizer.decode(y[0], skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
        "print(generated_code)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIydvUSFO5wJ",
        "outputId": "884ca6bc-4334-4fe6-ff43-9f1ca5115417"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:1 for open-end generation.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPZSIuov2lu0bLlGvo8VTYH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}