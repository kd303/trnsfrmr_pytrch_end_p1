{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_DBPedia_torchtext.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kd303/trnsfrmr_pytrch_end_p1/blob/main/session_5_torchtext/assignment/Assignment_DBPedia_torchtext.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKb3KfJgzpun"
      },
      "source": [
        "from torchtext.datasets import DBpedia"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pXFnhhMzx4R",
        "outputId": "ade4a5c8-401c-4cac-9d4c-79b64779e982"
      },
      "source": [
        "help(DBpedia)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function DBpedia in module torchtext.datasets.dbpedia:\n",
            "\n",
            "DBpedia(root='.data', split=('train', 'test'))\n",
            "    DBpedia dataset\n",
            "    \n",
            "    Separately returns the train/test split\n",
            "    \n",
            "    Number of lines per split:\n",
            "        train: 560000\n",
            "    \n",
            "        test: 70000\n",
            "    \n",
            "    \n",
            "    Number of classes\n",
            "        14\n",
            "    \n",
            "    \n",
            "    Args:\n",
            "        root: Directory where the datasets are saved.\n",
            "            Default: .data\n",
            "        split: split or splits to be returned. Can be a string or tuple of strings.\n",
            "            Default: ('train', 'test')\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3sWLqLmz2tp",
        "outputId": "2d46f575-40c8-4667-e480-d937ef16bd74"
      },
      "source": [
        "train_iter = DBpedia(split='train')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 68.3M/68.3M [00:00<00:00, 71.9MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ouX7lqki0HbA",
        "outputId": "b5101970-8640-4e41-e78c-8bb915032611"
      },
      "source": [
        "type(DBpedia(split='train'))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torchtext.data.datasets_utils._RawTextIterableDataset"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ev1nEGyC0JVd",
        "outputId": "cfdc0379-d29b-4187-8225-72e6e5b55aec"
      },
      "source": [
        "next(train_iter)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " 'E. D. Abbott Ltd  Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrB4PfQA0MQi",
        "outputId": "5b16e872-ba74-40cb-d24c-fe62036d98e1"
      },
      "source": [
        "for (line_number, (label, line)) in enumerate(train_iter):\n",
        "  print(label, line)\n",
        "  if line_number == 19:\n",
        "    break"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 Schwan-Stabilo  Schwan-STABILO is a German maker of pens for writing colouring and cosmetics as well as markers and highlighters for office use. It is the world's largest manufacturer of highlighter pens Stabilo Boss.\n",
            "1 Q-workshop  Q-workshop is a Polish company located in Poznań that specializes in designand production of polyhedral dice and dice accessories for use in various games (role-playing gamesboard games and tabletop wargames). They also run an online retail store and maintainan active forum community.Q-workshop was established in 2001 by Patryk Strzelewicz – a student from Poznań. Initiallythe company sold its products via online auction services but in 2005 a website and online store wereestablished.\n",
            "1 Marvell Software Solutions Israel  Marvell Software Solutions Israel known as RADLAN Computer Communications Limited before 2007 is a wholly owned subsidiary of Marvell Technology Group that specializes in local area network (LAN) technologies.\n",
            "1 Bergan Mercy Medical Center  Bergan Mercy Medical Center is a hospital located in Omaha Nebraska. It is part of the Alegent Health System.\n",
            "1 The Unsigned Guide  The Unsigned Guide is an online contacts directory and careers guide for the UK music industry. Founded in 2003 and first published as a printed directory The Unsigned Guide became an online only resource in November 2011.\n",
            "1 Rest of the world  Within sports and games played at the international competitive level the Rest of the World refers to a team of players from many countries of origin that compete against a single individual or a team from a single group such as a club or country. The team was formed in 1998.\n",
            "1 Globoforce  Globoforce is a multinational company co-headquartered in Southborough Massachusetts and Dublin Ireland providing cloud-based (software as a service) human capital management (HCM) software solutions. Its social recognition solutions are designed for employees to recognize and reward each other as incentive for performance and behaviors that are mapped to company values. A private corporation Globoforce is co-headquartered in Southborough Massachusetts and Dublin Ireland.\n",
            "1 Rompetrol  The Rompetrol Group N.V. is a Romanian oil company that operates in many countries throughout Europe. The group is active primarily in refining marketing and trading with additional operations in exploration and production and other oil industry services such as drilling EPCM and transportation.\n",
            "1 Wave Accounting  Wave is the brand name for a suite of online small business software products. The legal company name is Wave Accounting Inc. Wave is headquartered in the Leslieville neighbourhood in Toronto Canada and is currently being used in 200 countries.The company's first product was a free online accounting software designed for businesses with 1-9 employees.\n",
            "1 Angstrem (company)  Angstrem Group (Russian: ОАО «Ангстрем» named after angstrom) is a group of Russian companies one of the largest manufacturers of integrated circuits in Eastern Europe.The group includes: OAO Angstrem (the parent company design and manufacturing of electronic products and semiconductors); OAO Angstrem-M (custom design of integrated circuits staff training); OAO Angstrem-T (under-construction plant with 130-90 nm topology); OAO Angstrem-2M NGO Angstrem OAO Antek↑\n",
            "1 I-innovate (UK)  I-innovate (UK) is a London-based independent record label that diversified from video production into music management from 2009. I-innovate was founded by Najero Okenabirhie in 2008. I-innovate work with freelance directors marketers and artists in music and graphic design providing ad hoc services for clients labels and music professionals.\n",
            "1 JVC  Victor Company of Japan Ltd (日本ビクター株式会社 Nippon Bikutā Kabushiki-gaisha) TYO: 6792 usually referred to as JVC is a Japanese international consumer and professional electronics corporation based in Yokohama Japan. Founded in 1927 the company is best known for introducing Japan's first televisions and for developing the Video Home System (VHS) video recorder. In 2008 JVC merged with Kenwood Corporation to create JVC Kenwood Holdings.\n",
            "1 Toei Bus  The Toei Bus (都営バス Toei Basu) is a bus service operated by the Bus Service Division the Tokyo Metropolitan Bureau of Transportation (東京都交通局 Tōkyō-to Kōtsū-kyoku). It is also called To Bus (都バス To Basu).The bureau mainly operates bus routes in the special wards of Tokyo as well as those in the city of Ōme in the western Tama Area.\n",
            "1 Tear Drop Records  Tear Drop Records was a record label founded in Winnie Texas in the early 1960s by recording pioneer and radio personality Huey P Meaux. As a deejay Meaux was known as the Crazy Cajun a name that stuck with him throughout his long music career. In 1964 Meaux moved his Tear Drop label and his Crazy Cajun Enterprises to Conroe Texas where he partnered with a seasoned record producer Foy Lee.\n",
            "1 Presses polytechniques et universitaires romandes  The Presses polytechniques et universitaires romandes (PPUR literally Polytechnic and university press of French-speaking Switzerland) is a Swiss academic publishing house.It is based in Lausanne on the Lausanne campus in the Rolex Learning Center.The Presses polytechniques et universitaires romandes has an English-language imprint called EPFL Press.\n",
            "1 Websense  Websense is a San Diego-based company specializing in computer security softwarewhich is used by businesses and government institutions to protect their networks from cybercrime malware stop data theft prevent users from viewing sexual or other inappropriate content and discourage employees from spending time browsing non business-related websites.\n",
            "1 Adventist Health System  The Adventist Health System is a non-profit health care organization which operates facilities within the Southern and Midwestern regions of the United States. It is run by the Seventh-day Adventist Church.As of 2014 the system supports 44 hospitals and claims to be the largest not-for-profit Protestant healthcare provider in the nation.\n",
            "1 CIB Bank  CIB Bank is the second-biggest commercial bank in Hungary after the 1 January 2008 merger with Inter-Európa Bank. This follows the 2007 merger of their respective Italian parent companies Banca Intesa and Sanpaolo IMI to form Intesa Sanpaolo.\n",
            "1 Orfanato Music Group  Orfanato Music Group (OMG) is a Puerto Rican record label in the music and performance production industry. OMG was created by William Landron also known as Don Omar in 2007. Orfanato Music Group is an independent record label which represents artists. Its music repertoire is represented nationally and internationally within the Latino marketplace. Its strategy is to serve those interested in Latino music including reggaeton Latin hip hop and bachata amongst others.\n",
            "1 SCAN Health Plan  SCAN Health Plan (SCAN) is a not-for-profit health plan founded in 1977 and based in Long Beach California. The organization serves more than 110000 people with Medicare in Kern Los Angeles Orange Riverside San Bernardino San Diego and Ventura counties California and Maricopa county Arizona. The company also offers a health plan for Medicare and Medicaid-eligible individuals as part of the state’s long term care program in Maricopa county.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q7LwnrU-0cMI",
        "outputId": "d3cac4f9-e960-4ccb-92c7-487f97858928"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "train_iter = DBpedia(split = 'train')\n",
        "\n",
        "help(DataLoader)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class DataLoader in module torch.utils.data.dataloader:\n",
            "\n",
            "class DataLoader(typing.Generic)\n",
            " |  DataLoader(*args, **kwds)\n",
            " |  \n",
            " |  Data loader. Combines a dataset and a sampler, and provides an iterable over\n",
            " |  the given dataset.\n",
            " |  \n",
            " |  The :class:`~torch.utils.data.DataLoader` supports both map-style and\n",
            " |  iterable-style datasets with single- or multi-process loading, customizing\n",
            " |  loading order and optional automatic batching (collation) and memory pinning.\n",
            " |  \n",
            " |  See :py:mod:`torch.utils.data` documentation page for more details.\n",
            " |  \n",
            " |  Args:\n",
            " |      dataset (Dataset): dataset from which to load the data.\n",
            " |      batch_size (int, optional): how many samples per batch to load\n",
            " |          (default: ``1``).\n",
            " |      shuffle (bool, optional): set to ``True`` to have the data reshuffled\n",
            " |          at every epoch (default: ``False``).\n",
            " |      sampler (Sampler or Iterable, optional): defines the strategy to draw\n",
            " |          samples from the dataset. Can be any ``Iterable`` with ``__len__``\n",
            " |          implemented. If specified, :attr:`shuffle` must not be specified.\n",
            " |      batch_sampler (Sampler or Iterable, optional): like :attr:`sampler`, but\n",
            " |          returns a batch of indices at a time. Mutually exclusive with\n",
            " |          :attr:`batch_size`, :attr:`shuffle`, :attr:`sampler`,\n",
            " |          and :attr:`drop_last`.\n",
            " |      num_workers (int, optional): how many subprocesses to use for data\n",
            " |          loading. ``0`` means that the data will be loaded in the main process.\n",
            " |          (default: ``0``)\n",
            " |      collate_fn (callable, optional): merges a list of samples to form a\n",
            " |          mini-batch of Tensor(s).  Used when using batched loading from a\n",
            " |          map-style dataset.\n",
            " |      pin_memory (bool, optional): If ``True``, the data loader will copy Tensors\n",
            " |          into CUDA pinned memory before returning them.  If your data elements\n",
            " |          are a custom type, or your :attr:`collate_fn` returns a batch that is a custom type,\n",
            " |          see the example below.\n",
            " |      drop_last (bool, optional): set to ``True`` to drop the last incomplete batch,\n",
            " |          if the dataset size is not divisible by the batch size. If ``False`` and\n",
            " |          the size of dataset is not divisible by the batch size, then the last batch\n",
            " |          will be smaller. (default: ``False``)\n",
            " |      timeout (numeric, optional): if positive, the timeout value for collecting a batch\n",
            " |          from workers. Should always be non-negative. (default: ``0``)\n",
            " |      worker_init_fn (callable, optional): If not ``None``, this will be called on each\n",
            " |          worker subprocess with the worker id (an int in ``[0, num_workers - 1]``) as\n",
            " |          input, after seeding and before data loading. (default: ``None``)\n",
            " |      generator (torch.Generator, optional): If not ``None``, this RNG will be used\n",
            " |          by RandomSampler to generate random indexes and multiprocessing to generate\n",
            " |          `base_seed` for workers. (default: ``None``)\n",
            " |      prefetch_factor (int, optional, keyword-only arg): Number of samples loaded\n",
            " |          in advance by each worker. ``2`` means there will be a total of\n",
            " |          2 * num_workers samples prefetched across all workers. (default: ``2``)\n",
            " |      persistent_workers (bool, optional): If ``True``, the data loader will not shutdown\n",
            " |          the worker processes after a dataset has been consumed once. This allows to\n",
            " |          maintain the workers `Dataset` instances alive. (default: ``False``)\n",
            " |  \n",
            " |  \n",
            " |  .. warning:: If the ``spawn`` start method is used, :attr:`worker_init_fn`\n",
            " |               cannot be an unpicklable object, e.g., a lambda function. See\n",
            " |               :ref:`multiprocessing-best-practices` on more details related\n",
            " |               to multiprocessing in PyTorch.\n",
            " |  \n",
            " |  .. warning:: ``len(dataloader)`` heuristic is based on the length of the sampler used.\n",
            " |               When :attr:`dataset` is an :class:`~torch.utils.data.IterableDataset`,\n",
            " |               it instead returns an estimate based on ``len(dataset) / batch_size``, with proper\n",
            " |               rounding depending on :attr:`drop_last`, regardless of multi-process loading\n",
            " |               configurations. This represents the best guess PyTorch can make because PyTorch\n",
            " |               trusts user :attr:`dataset` code in correctly handling multi-process\n",
            " |               loading to avoid duplicate data.\n",
            " |  \n",
            " |               However, if sharding results in multiple workers having incomplete last batches,\n",
            " |               this estimate can still be inaccurate, because (1) an otherwise complete batch can\n",
            " |               be broken into multiple ones and (2) more than one batch worth of samples can be\n",
            " |               dropped when :attr:`drop_last` is set. Unfortunately, PyTorch can not detect such\n",
            " |               cases in general.\n",
            " |  \n",
            " |               See `Dataset Types`_ for more details on these two types of datasets and how\n",
            " |               :class:`~torch.utils.data.IterableDataset` interacts with\n",
            " |               `Multi-process data loading`_.\n",
            " |  \n",
            " |  .. warning:: See :ref:`reproducibility`, and :ref:`dataloader-workers-random-seed`, and\n",
            " |               :ref:`data-loading-randomness` notes for random seed related questions.\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      DataLoader\n",
            " |      typing.Generic\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, dataset: torch.utils.data.dataset.Dataset[+T_co], batch_size: Union[int, NoneType] = 1, shuffle: bool = False, sampler: Union[torch.utils.data.sampler.Sampler, NoneType] = None, batch_sampler: Union[torch.utils.data.sampler.Sampler[Sequence], NoneType] = None, num_workers: int = 0, collate_fn: Union[Callable[[List[~T]], Any], NoneType] = None, pin_memory: bool = False, drop_last: bool = False, timeout: float = 0, worker_init_fn: Union[Callable[[int], NoneType], NoneType] = None, multiprocessing_context=None, generator=None, *, prefetch_factor: int = 2, persistent_workers: bool = False)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  __iter__(self) -> '_BaseDataLoaderIter'\n",
            " |      # We quote '_BaseDataLoaderIter' since it isn't defined yet and the definition can't be moved up\n",
            " |      # since '_BaseDataLoaderIter' references 'DataLoader'.\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |  \n",
            " |  __setattr__(self, attr, val)\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  check_worker_number_rationality(self)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  multiprocessing_context\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __annotations__ = {'_iterator': typing.Union[ForwardRef('_BaseDataLoad...\n",
            " |  \n",
            " |  __orig_bases__ = (typing.Generic[+T_co],)\n",
            " |  \n",
            " |  __parameters__ = (+T_co,)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Class methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __class_getitem__(params) from builtins.type\n",
            " |  \n",
            " |  __init_subclass__(*args, **kwargs) from builtins.type\n",
            " |      This method is called when a class is subclassed.\n",
            " |      \n",
            " |      The default implementation does nothing. It may be\n",
            " |      overridden to extend subclasses.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Static methods inherited from typing.Generic:\n",
            " |  \n",
            " |  __new__(cls, *args, **kwds)\n",
            " |      Create and return a new object.  See help(type) for accurate signature.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_vsHPV_1TZ-"
      },
      "source": [
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_4z8hD41_fI",
        "outputId": "dd467e16-2208-43ba-edd3-c24f0eeacd08"
      },
      "source": [
        "next(iter(dataloader))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " ('E. D. Abbott Ltd  Abbott of Farnham E D Abbott Limited was a British coachbuilding business based in Farnham Surrey trading under that name from 1929. A major part of their output was under sub-contract to motor vehicle manufacturers. Their business closed in 1972.',\n",
              "  \"Schwan-Stabilo  Schwan-STABILO is a German maker of pens for writing colouring and cosmetics as well as markers and highlighters for office use. It is the world's largest manufacturer of highlighter pens Stabilo Boss.\",\n",
              "  'Q-workshop  Q-workshop is a Polish company located in Poznań that specializes in designand production of polyhedral dice and dice accessories for use in various games (role-playing gamesboard games and tabletop wargames). They also run an online retail store and maintainan active forum community.Q-workshop was established in 2001 by Patryk Strzelewicz – a student from Poznań. Initiallythe company sold its products via online auction services but in 2005 a website and online store wereestablished.',\n",
              "  'Marvell Software Solutions Israel  Marvell Software Solutions Israel known as RADLAN Computer Communications Limited before 2007 is a wholly owned subsidiary of Marvell Technology Group that specializes in local area network (LAN) technologies.',\n",
              "  'Bergan Mercy Medical Center  Bergan Mercy Medical Center is a hospital located in Omaha Nebraska. It is part of the Alegent Health System.',\n",
              "  'The Unsigned Guide  The Unsigned Guide is an online contacts directory and careers guide for the UK music industry. Founded in 2003 and first published as a printed directory The Unsigned Guide became an online only resource in November 2011.',\n",
              "  'Rest of the world  Within sports and games played at the international competitive level the Rest of the World refers to a team of players from many countries of origin that compete against a single individual or a team from a single group such as a club or country. The team was formed in 1998.',\n",
              "  'Globoforce  Globoforce is a multinational company co-headquartered in Southborough Massachusetts and Dublin Ireland providing cloud-based (software as a service) human capital management (HCM) software solutions. Its social recognition solutions are designed for employees to recognize and reward each other as incentive for performance and behaviors that are mapped to company values. A private corporation Globoforce is co-headquartered in Southborough Massachusetts and Dublin Ireland.')]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zq-IuEKY2GQR"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.vocab import build_vocab_from_iterator"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DdxunJbc3FUH",
        "outputId": "5e08f965-18ca-43df-bb68-ebc58d3c011f"
      },
      "source": [
        "help(get_tokenizer)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function get_tokenizer in module torchtext.data.utils:\n",
            "\n",
            "get_tokenizer(tokenizer, language='en')\n",
            "    Generate tokenizer function for a string sentence.\n",
            "    \n",
            "    Args:\n",
            "        tokenizer: the name of tokenizer function. If None, it returns split()\n",
            "            function, which splits the string sentence by space.\n",
            "            If basic_english, it returns _basic_english_normalize() function,\n",
            "            which normalize the string first and split by space. If a callable\n",
            "            function, it will return the function. If a tokenizer library\n",
            "            (e.g. spacy, moses, toktok, revtok, subword), it returns the\n",
            "            corresponding library.\n",
            "        language: Default en\n",
            "    \n",
            "    Examples:\n",
            "        >>> import torchtext\n",
            "        >>> from torchtext.data import get_tokenizer\n",
            "        >>> tokenizer = get_tokenizer(\"basic_english\")\n",
            "        >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
            "        >>> tokens\n",
            "        >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_y2YR4UV3GOi"
      },
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iNW8jB3h3cay",
        "outputId": "2c8fb0a7-fd55-480a-9bc9-b779e90acd90"
      },
      "source": [
        "tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
        "tokens"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZKg_ePFP3emp",
        "outputId": "246d0728-7d88-4473-96ce-5faffbf68e90"
      },
      "source": [
        "help(build_vocab_from_iterator)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on function build_vocab_from_iterator in module torchtext.vocab.vocab_factory:\n",
            "\n",
            "build_vocab_from_iterator(iterator: Iterable, min_freq: int = 1, specials: Union[List[str], NoneType] = None, special_first: bool = True) -> torchtext.vocab.vocab.Vocab\n",
            "    Build a Vocab from an iterator.\n",
            "    \n",
            "    Args:\n",
            "        iterator: Iterator used to build Vocab. Must yield list or iterator of tokens.\n",
            "        min_freq: The minimum frequency needed to include a token in the vocabulary.\n",
            "        specials: Special symbols to add. The order of supplied tokens will be preserved.\n",
            "        special_first: Indicates whether to insert symbols at the beginning or at the end.\n",
            "    \n",
            "    \n",
            "    Returns:\n",
            "        torchtext.vocab.Vocab: A `Vocab` object\n",
            "    \n",
            "    Examples:\n",
            "        >>> #generating vocab from text file\n",
            "        >>> import io\n",
            "        >>> from torchtext.vocab import build_vocab_from_iterator\n",
            "        >>> def yield_tokens(file_path):\n",
            "        >>>     with io.open(file_path, encoding = 'utf-8') as f:\n",
            "        >>>         for line in f:\n",
            "        >>>             yield line.strip().split()\n",
            "        >>> vocab = build_vocab_from_iterator(yield_tokens_batch(file_path), specials=[\"<unk>\"])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oALqqil43mX2"
      },
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "train_iter = DBpedia(split = 'train')\n",
        "\n",
        "def yield_tokens(data_iter):\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"])"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-r4fhQZ85Piw"
      },
      "source": [
        "def my_func():\n",
        "  print(\"Hello! Welcome to the Dothraki Kingdon\")\n",
        "  yield\n",
        "  print(\"Have you brought some Gold for us?\")\n",
        "  yield\n",
        "  print(\"NO? Then you must die!!!\")"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4b6NXOg5VZY",
        "outputId": "1cfe8d23-1c61-4f84-fab5-6051d192d484"
      },
      "source": [
        "temp_my_func = my_func()\n",
        "next(temp_my_func)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! Welcome to the Dothraki Kingdon\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hHYpIkNx5khJ",
        "outputId": "9904b348-b852-4d30-86bd-e921d16b36b8"
      },
      "source": [
        "next(temp_my_func)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Have you brought some Gold for us?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM6Rneo55rGS"
      },
      "source": [
        "## next(temp_my_func)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6j3Ug97O6VHX",
        "outputId": "3bbcbfd6-2a43-437a-dfbf-893c7a9406cd"
      },
      "source": [
        "help(vocab)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on Vocab in module torchtext.vocab.vocab object:\n",
            "\n",
            "class Vocab(torch.nn.modules.module.Module)\n",
            " |  Vocab(vocab)\n",
            " |  \n",
            " |  Base class for all neural network modules.\n",
            " |  \n",
            " |  Your models should also subclass this class.\n",
            " |  \n",
            " |  Modules can also contain other Modules, allowing to nest them in\n",
            " |  a tree structure. You can assign the submodules as regular attributes::\n",
            " |  \n",
            " |      import torch.nn as nn\n",
            " |      import torch.nn.functional as F\n",
            " |  \n",
            " |      class Model(nn.Module):\n",
            " |          def __init__(self):\n",
            " |              super(Model, self).__init__()\n",
            " |              self.conv1 = nn.Conv2d(1, 20, 5)\n",
            " |              self.conv2 = nn.Conv2d(20, 20, 5)\n",
            " |  \n",
            " |          def forward(self, x):\n",
            " |              x = F.relu(self.conv1(x))\n",
            " |              return F.relu(self.conv2(x))\n",
            " |  \n",
            " |  Submodules assigned in this way will be registered, and will have their\n",
            " |  parameters converted too when you call :meth:`to`, etc.\n",
            " |  \n",
            " |  :ivar training: Boolean represents whether this module is in training or\n",
            " |                  evaluation mode.\n",
            " |  :vartype training: bool\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      Vocab\n",
            " |      torch.nn.modules.module.Module\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __contains__(self, token: str) -> bool\n",
            " |      Args:\n",
            " |          token: The token for which to check the membership.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Whether the token is member of vocab or not.\n",
            " |  \n",
            " |  __getitem__(self, token: str) -> int\n",
            " |      Args:\n",
            " |          token: The token used to lookup the corresponding index.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The index corresponding to the associated token.\n",
            " |  \n",
            " |  __init__(self, vocab)\n",
            " |      Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
            " |  \n",
            " |  __len__(self) -> int\n",
            " |      Returns:\n",
            " |          The length of the vocab.\n",
            " |  \n",
            " |  __prepare_scriptable__(self)\n",
            " |      Return a JITable Vocab.\n",
            " |  \n",
            " |  append_token(self, token: str) -> None\n",
            " |      Args:\n",
            " |          token: The token used to lookup the corresponding index.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `token` already exists in the vocab\n",
            " |  \n",
            " |  forward(self, tokens: List[str]) -> List[int]\n",
            " |      Calls the `lookup_indices` method\n",
            " |      \n",
            " |      Args:\n",
            " |          tokens: a list of tokens used to lookup their corresponding `indices`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The indices associated with a list of `tokens`.\n",
            " |  \n",
            " |  get_default_index(self) -> Union[int, NoneType]\n",
            " |      Returns:\n",
            " |          Value of default index if it is set.\n",
            " |  \n",
            " |  get_itos(self) -> List[str]\n",
            " |      Returns:\n",
            " |          List mapping indices to tokens.\n",
            " |  \n",
            " |  get_stoi(self) -> Dict[str, int]\n",
            " |      Returns:\n",
            " |          Dictionary mapping tokens to indices.\n",
            " |  \n",
            " |  insert_token(self, token: str, index: int) -> None\n",
            " |      Args:\n",
            " |          token: The token used to lookup the corresponding index.\n",
            " |          index: The index corresponding to the associated token.\n",
            " |      Raises:\n",
            " |          RuntimeError: If `index` is not in range [0, Vocab.size()] or if `token` already exists in the vocab.\n",
            " |  \n",
            " |  lookup_indices(self, tokens: List[str]) -> List[int]\n",
            " |      Args:\n",
            " |          tokens: the tokens used to lookup their corresponding `indices`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The 'indices` associated with `tokens`.\n",
            " |  \n",
            " |  lookup_token(self, index: int) -> str\n",
            " |      Args:\n",
            " |          index: The index corresponding to the associated token.\n",
            " |      \n",
            " |      Returns:\n",
            " |          token: The token used to lookup the corresponding index.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If `index` not in range [0, itos.size()).\n",
            " |  \n",
            " |  lookup_tokens(self, indices: List[int]) -> List[str]\n",
            " |      Args:\n",
            " |          indices: The `indices` used to lookup their corresponding`tokens`.\n",
            " |      \n",
            " |      Returns:\n",
            " |          The `tokens` associated with `indices`.\n",
            " |      \n",
            " |      Raises:\n",
            " |          RuntimeError: If an index within `indices` is not int range [0, itos.size()).\n",
            " |  \n",
            " |  set_default_index(self, index: Union[int, NoneType]) -> None\n",
            " |      Args:\n",
            " |          index: Value of default index. This index will be returned when OOV token is queried.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors defined here:\n",
            " |  \n",
            " |  is_jitable\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __jit_unused_properties__ = ['is_jitable']\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __call__ = _call_impl(self, *input, **kwargs)\n",
            " |  \n",
            " |  __delattr__(self, name)\n",
            " |      Implement delattr(self, name).\n",
            " |  \n",
            " |  __dir__(self)\n",
            " |      Default dir() implementation.\n",
            " |  \n",
            " |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]\n",
            " |  \n",
            " |  __repr__(self)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setattr__(self, name: str, value: Union[torch.Tensor, ForwardRef('Module')]) -> None\n",
            " |      Implement setattr(self, name, value).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  add_module(self, name: str, module: Union[ForwardRef('Module'), NoneType]) -> None\n",
            " |      Adds a child module to the current module.\n",
            " |      \n",
            " |      The module can be accessed as an attribute using the given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (string): name of the child module. The child module can be\n",
            " |              accessed from this module using the given name\n",
            " |          module (Module): child module to be added to the module.\n",
            " |  \n",
            " |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T\n",
            " |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)\n",
            " |      as well as self. Typical use includes initializing the parameters of a model\n",
            " |      (see also :ref:`nn-init-doc`).\n",
            " |      \n",
            " |      Args:\n",
            " |          fn (:class:`Module` -> None): function to be applied to each submodule\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> @torch.no_grad()\n",
            " |          >>> def init_weights(m):\n",
            " |          >>>     print(m)\n",
            " |          >>>     if type(m) == nn.Linear:\n",
            " |          >>>         m.weight.fill_(1.0)\n",
            " |          >>>         print(m.weight)\n",
            " |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
            " |          >>> net.apply(init_weights)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 1.,  1.],\n",
            " |                  [ 1.,  1.]])\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 1.,  1.],\n",
            " |                  [ 1.,  1.]])\n",
            " |          Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |          Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |  \n",
            " |  bfloat16(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]\n",
            " |      Returns an iterator over module buffers.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          torch.Tensor: module buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for buf in model.buffers():\n",
            " |          >>>     print(type(buf), buf.size())\n",
            " |          <class 'torch.Tensor'> (20L,)\n",
            " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  children(self) -> Iterator[ForwardRef('Module')]\n",
            " |      Returns an iterator over immediate children modules.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a child module\n",
            " |  \n",
            " |  cpu(self: ~T) -> ~T\n",
            " |      Moves all model parameters and buffers to the CPU.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Moves all model parameters and buffers to the GPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on GPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  double(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``double`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  eval(self: ~T) -> ~T\n",
            " |      Sets the module in evaluation mode.\n",
            " |      \n",
            " |      This has any effect only on certain modules. See documentations of\n",
            " |      particular modules for details of their behaviors in training/evaluation\n",
            " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
            " |      etc.\n",
            " |      \n",
            " |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.\n",
            " |      \n",
            " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
            " |      `.eval()` and several similar mechanisms that may be confused with it.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  extra_repr(self) -> str\n",
            " |      Set the extra representation of the module\n",
            " |      \n",
            " |      To print customized extra information, you should re-implement\n",
            " |      this method in your own modules. Both single-line and multi-line\n",
            " |      strings are acceptable.\n",
            " |  \n",
            " |  float(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``float`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  get_buffer(self, target: str) -> 'Tensor'\n",
            " |      Returns the buffer given by ``target`` if it exists,\n",
            " |      otherwise throws an error.\n",
            " |      \n",
            " |      See the docstring for ``get_submodule`` for a more detailed\n",
            " |      explanation of this method's functionality as well as how to\n",
            " |      correctly specify ``target``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the buffer\n",
            " |              to look for. (See ``get_submodule`` for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.Tensor: The buffer referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not a\n",
            " |              buffer\n",
            " |  \n",
            " |  get_extra_state(self) -> Any\n",
            " |      Returns any extra state to include in the module's state_dict.\n",
            " |      Implement this and a corresponding :func:`set_extra_state` for your module\n",
            " |      if you need to store extra state. This function is called when building the\n",
            " |      module's `state_dict()`.\n",
            " |      \n",
            " |      Note that extra state should be pickleable to ensure working serialization\n",
            " |      of the state_dict. We only provide provide backwards compatibility guarantees\n",
            " |      for serializing Tensors; other objects may break backwards compatibility if\n",
            " |      their serialized pickled form changes.\n",
            " |      \n",
            " |      Returns:\n",
            " |          object: Any extra state to store in the module's state_dict\n",
            " |  \n",
            " |  get_parameter(self, target: str) -> 'Parameter'\n",
            " |      Returns the parameter given by ``target`` if it exists,\n",
            " |      otherwise throws an error.\n",
            " |      \n",
            " |      See the docstring for ``get_submodule`` for a more detailed\n",
            " |      explanation of this method's functionality as well as how to\n",
            " |      correctly specify ``target``.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the Parameter\n",
            " |              to look for. (See ``get_submodule`` for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.nn.Parameter: The Parameter referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Parameter``\n",
            " |  \n",
            " |  get_submodule(self, target: str) -> 'Module'\n",
            " |      Returns the submodule given by ``target`` if it exists,\n",
            " |      otherwise throws an error.\n",
            " |      \n",
            " |      For example, let's say you have an ``nn.Module`` ``A`` that\n",
            " |      looks like this:\n",
            " |      \n",
            " |      .. code-block::text\n",
            " |      \n",
            " |          A(\n",
            " |              (net_b): Module(\n",
            " |                  (net_c): Module(\n",
            " |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))\n",
            " |                  )\n",
            " |                  (linear): Linear(in_features=100, out_features=200, bias=True)\n",
            " |              )\n",
            " |          )\n",
            " |      \n",
            " |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested\n",
            " |      submodule ``net_b``, which itself has two submodules ``net_c``\n",
            " |      and ``linear``. ``net_c`` then has a submodule ``conv``.)\n",
            " |      \n",
            " |      To check whether or not we have the ``linear`` submodule, we\n",
            " |      would call ``get_submodule(\"net_b.linear\")``. To check whether\n",
            " |      we have the ``conv`` submodule, we would call\n",
            " |      ``get_submodule(\"net_b.net_c.conv\")``.\n",
            " |      \n",
            " |      The runtime of ``get_submodule`` is bounded by the degree\n",
            " |      of module nesting in ``target``. A query against\n",
            " |      ``named_modules`` achieves the same result, but it is O(N) in\n",
            " |      the number of transitive modules. So, for a simple check to see\n",
            " |      if some submodule exists, ``get_submodule`` should always be\n",
            " |      used.\n",
            " |      \n",
            " |      Args:\n",
            " |          target: The fully-qualified string name of the submodule\n",
            " |              to look for. (See above example for how to specify a\n",
            " |              fully-qualified string.)\n",
            " |      \n",
            " |      Returns:\n",
            " |          torch.nn.Module: The submodule referenced by ``target``\n",
            " |      \n",
            " |      Raises:\n",
            " |          AttributeError: If the target string references an invalid\n",
            " |              path or resolves to something that is not an\n",
            " |              ``nn.Module``\n",
            " |  \n",
            " |  half(self: ~T) -> ~T\n",
            " |      Casts all floating point parameters and buffers to ``half`` datatype.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  load_state_dict(self, state_dict: 'OrderedDict[str, Tensor]', strict: bool = True)\n",
            " |      Copies parameters and buffers from :attr:`state_dict` into\n",
            " |      this module and its descendants. If :attr:`strict` is ``True``, then\n",
            " |      the keys of :attr:`state_dict` must exactly match the keys returned\n",
            " |      by this module's :meth:`~torch.nn.Module.state_dict` function.\n",
            " |      \n",
            " |      Args:\n",
            " |          state_dict (dict): a dict containing parameters and\n",
            " |              persistent buffers.\n",
            " |          strict (bool, optional): whether to strictly enforce that the keys\n",
            " |              in :attr:`state_dict` match the keys returned by this module's\n",
            " |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``\n",
            " |      \n",
            " |      Returns:\n",
            " |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:\n",
            " |              * **missing_keys** is a list of str containing the missing keys\n",
            " |              * **unexpected_keys** is a list of str containing the unexpected keys\n",
            " |      \n",
            " |      Note:\n",
            " |          If a parameter or buffer is registered as ``None`` and its corresponding key\n",
            " |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a\n",
            " |          ``RuntimeError``.\n",
            " |  \n",
            " |  modules(self) -> Iterator[ForwardRef('Module')]\n",
            " |      Returns an iterator over all modules in the network.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Module: a module in the network\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.modules()):\n",
            " |                  print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          )\n",
            " |          1 -> Linear(in_features=2, out_features=2, bias=True)\n",
            " |  \n",
            " |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]\n",
            " |      Returns an iterator over module buffers, yielding both the\n",
            " |      name of the buffer as well as the buffer itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all buffer names.\n",
            " |          recurse (bool): if True, then yields buffers of this module\n",
            " |              and all submodules. Otherwise, yields only buffers that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, torch.Tensor): Tuple containing the name and buffer\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for name, buf in self.named_buffers():\n",
            " |          >>>    if name in ['running_var']:\n",
            " |          >>>        print(buf.size())\n",
            " |  \n",
            " |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]\n",
            " |      Returns an iterator over immediate children modules, yielding both\n",
            " |      the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, Module): Tuple containing a name and child module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for name, module in model.named_children():\n",
            " |          >>>     if name in ['conv4', 'conv5']:\n",
            " |          >>>         print(module)\n",
            " |  \n",
            " |  named_modules(self, memo: Union[Set[ForwardRef('Module')], NoneType] = None, prefix: str = '', remove_duplicate: bool = True)\n",
            " |      Returns an iterator over all modules in the network, yielding\n",
            " |      both the name of the module as well as the module itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          memo: a memo to store the set of modules already added to the result\n",
            " |          prefix: a prefix that will be added to the name of the module\n",
            " |          remove_duplicate: whether to remove the duplicated module instances in the result\n",
            " |          or not\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, Module): Tuple of name and module\n",
            " |      \n",
            " |      Note:\n",
            " |          Duplicate modules are returned only once. In the following\n",
            " |          example, ``l`` will be returned only once.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> l = nn.Linear(2, 2)\n",
            " |          >>> net = nn.Sequential(l, l)\n",
            " |          >>> for idx, m in enumerate(net.named_modules()):\n",
            " |                  print(idx, '->', m)\n",
            " |      \n",
            " |          0 -> ('', Sequential(\n",
            " |            (0): Linear(in_features=2, out_features=2, bias=True)\n",
            " |            (1): Linear(in_features=2, out_features=2, bias=True)\n",
            " |          ))\n",
            " |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))\n",
            " |  \n",
            " |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]\n",
            " |      Returns an iterator over module parameters, yielding both the\n",
            " |      name of the parameter as well as the parameter itself.\n",
            " |      \n",
            " |      Args:\n",
            " |          prefix (str): prefix to prepend to all parameter names.\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          (string, Parameter): Tuple containing the name and parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for name, param in self.named_parameters():\n",
            " |          >>>    if name in ['bias']:\n",
            " |          >>>        print(param.size())\n",
            " |  \n",
            " |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]\n",
            " |      Returns an iterator over module parameters.\n",
            " |      \n",
            " |      This is typically passed to an optimizer.\n",
            " |      \n",
            " |      Args:\n",
            " |          recurse (bool): if True, then yields parameters of this module\n",
            " |              and all submodules. Otherwise, yields only parameters that\n",
            " |              are direct members of this module.\n",
            " |      \n",
            " |      Yields:\n",
            " |          Parameter: module parameter\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> for param in model.parameters():\n",
            " |          >>>     print(type(param), param.size())\n",
            " |          <class 'torch.Tensor'> (20L,)\n",
            " |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)\n",
            " |  \n",
            " |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
            " |      Registers a backward hook on the module.\n",
            " |      \n",
            " |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and\n",
            " |      the behavior of this function will change in future versions.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_buffer(self, name: str, tensor: Union[torch.Tensor, NoneType], persistent: bool = True) -> None\n",
            " |      Adds a buffer to the module.\n",
            " |      \n",
            " |      This is typically used to register a buffer that should not to be\n",
            " |      considered a model parameter. For example, BatchNorm's ``running_mean``\n",
            " |      is not a parameter, but is part of the module's state. Buffers, by\n",
            " |      default, are persistent and will be saved alongside parameters. This\n",
            " |      behavior can be changed by setting :attr:`persistent` to ``False``. The\n",
            " |      only difference between a persistent buffer and a non-persistent buffer\n",
            " |      is that the latter will not be a part of this module's\n",
            " |      :attr:`state_dict`.\n",
            " |      \n",
            " |      Buffers can be accessed as attributes using given names.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (string): name of the buffer. The buffer can be accessed\n",
            " |              from this module using the given name\n",
            " |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations\n",
            " |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,\n",
            " |              the buffer is **not** included in the module's :attr:`state_dict`.\n",
            " |          persistent (bool): whether the buffer is part of this module's\n",
            " |              :attr:`state_dict`.\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> self.register_buffer('running_mean', torch.zeros(num_features))\n",
            " |  \n",
            " |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
            " |      Registers a forward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time after :func:`forward` has computed an output.\n",
            " |      It should have the following signature::\n",
            " |      \n",
            " |          hook(module, input, output) -> None or modified output\n",
            " |      \n",
            " |      The input contains only the positional arguments given to the module.\n",
            " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
            " |      The hook can modify the output. It can modify the input inplace but\n",
            " |      it will not have effect on forward since this is called after\n",
            " |      :func:`forward` is called.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle\n",
            " |      Registers a forward pre-hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time before :func:`forward` is invoked.\n",
            " |      It should have the following signature::\n",
            " |      \n",
            " |          hook(module, input) -> None or modified input\n",
            " |      \n",
            " |      The input contains only the positional arguments given to the module.\n",
            " |      Keyword arguments won't be passed to the hooks and only to the ``forward``.\n",
            " |      The hook can modify the input. User can either return a tuple or a\n",
            " |      single modified value in the hook. We will wrap the value into a tuple\n",
            " |      if a single value is returned(unless that value is already a tuple).\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Union[NoneType, torch.Tensor]]) -> torch.utils.hooks.RemovableHandle\n",
            " |      Registers a backward hook on the module.\n",
            " |      \n",
            " |      The hook will be called every time the gradients with respect to module\n",
            " |      inputs are computed. The hook should have the following signature::\n",
            " |      \n",
            " |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None\n",
            " |      \n",
            " |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients\n",
            " |      with respect to the inputs and outputs respectively. The hook should\n",
            " |      not modify its arguments, but it can optionally return a new gradient with\n",
            " |      respect to the input that will be used in place of :attr:`grad_input` in\n",
            " |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given\n",
            " |      as positional arguments and all kwarg arguments are ignored. Entries\n",
            " |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor\n",
            " |      arguments.\n",
            " |      \n",
            " |      For technical reasons, when this hook is applied to a Module, its forward function will\n",
            " |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view\n",
            " |      of each Tensor returned by the Module's forward function.\n",
            " |      \n",
            " |      .. warning ::\n",
            " |          Modifying inputs or outputs inplace is not allowed when using backward hooks and\n",
            " |          will raise an error.\n",
            " |      \n",
            " |      Returns:\n",
            " |          :class:`torch.utils.hooks.RemovableHandle`:\n",
            " |              a handle that can be used to remove the added hook by calling\n",
            " |              ``handle.remove()``\n",
            " |  \n",
            " |  register_parameter(self, name: str, param: Union[torch.nn.parameter.Parameter, NoneType]) -> None\n",
            " |      Adds a parameter to the module.\n",
            " |      \n",
            " |      The parameter can be accessed as an attribute using given name.\n",
            " |      \n",
            " |      Args:\n",
            " |          name (string): name of the parameter. The parameter can be accessed\n",
            " |              from this module using the given name\n",
            " |          param (Parameter or None): parameter to be added to the module. If\n",
            " |              ``None``, then operations that run on parameters, such as :attr:`cuda`,\n",
            " |              are ignored. If ``None``, the parameter is **not** included in the\n",
            " |              module's :attr:`state_dict`.\n",
            " |  \n",
            " |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T\n",
            " |      Change if autograd should record operations on parameters in this\n",
            " |      module.\n",
            " |      \n",
            " |      This method sets the parameters' :attr:`requires_grad` attributes\n",
            " |      in-place.\n",
            " |      \n",
            " |      This method is helpful for freezing part of the module for finetuning\n",
            " |      or training parts of a model individually (e.g., GAN training).\n",
            " |      \n",
            " |      See :ref:`locally-disable-grad-doc` for a comparison between\n",
            " |      `.requires_grad_()` and several similar mechanisms that may be confused with it.\n",
            " |      \n",
            " |      Args:\n",
            " |          requires_grad (bool): whether autograd should record operations on\n",
            " |                                parameters in this module. Default: ``True``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  set_extra_state(self, state: Any)\n",
            " |      This function is called from :func:`load_state_dict` to handle any extra state\n",
            " |      found within the `state_dict`. Implement this function and a corresponding\n",
            " |      :func:`get_extra_state` for your module if you need to store extra state within its\n",
            " |      `state_dict`.\n",
            " |      \n",
            " |      Args:\n",
            " |          state (dict): Extra state from the `state_dict`\n",
            " |  \n",
            " |  share_memory(self: ~T) -> ~T\n",
            " |      See :meth:`torch.Tensor.share_memory_`\n",
            " |  \n",
            " |  state_dict(self, destination=None, prefix='', keep_vars=False)\n",
            " |      Returns a dictionary containing a whole state of the module.\n",
            " |      \n",
            " |      Both parameters and persistent buffers (e.g. running averages) are\n",
            " |      included. Keys are corresponding parameter and buffer names.\n",
            " |      Parameters and buffers set to ``None`` are not included.\n",
            " |      \n",
            " |      Returns:\n",
            " |          dict:\n",
            " |              a dictionary containing a whole state of the module\n",
            " |      \n",
            " |      Example::\n",
            " |      \n",
            " |          >>> module.state_dict().keys()\n",
            " |          ['bias', 'weight']\n",
            " |  \n",
            " |  to(self, *args, **kwargs)\n",
            " |      Moves and/or casts the parameters and buffers.\n",
            " |      \n",
            " |      This can be called as\n",
            " |      \n",
            " |      .. function:: to(device=None, dtype=None, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(dtype, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(tensor, non_blocking=False)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      .. function:: to(memory_format=torch.channels_last)\n",
            " |         :noindex:\n",
            " |      \n",
            " |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts\n",
            " |      floating point or complex :attr:`dtype`\\ s. In addition, this method will\n",
            " |      only cast the floating point or complex parameters and buffers to :attr:`dtype`\n",
            " |      (if given). The integral parameters and buffers will be moved\n",
            " |      :attr:`device`, if that is given, but with dtypes unchanged. When\n",
            " |      :attr:`non_blocking` is set, it tries to convert/move asynchronously\n",
            " |      with respect to the host if possible, e.g., moving CPU Tensors with\n",
            " |      pinned memory to CUDA devices.\n",
            " |      \n",
            " |      See below for examples.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): the desired device of the parameters\n",
            " |              and buffers in this module\n",
            " |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of\n",
            " |              the parameters and buffers in this module\n",
            " |          tensor (torch.Tensor): Tensor whose dtype and device are the desired\n",
            " |              dtype and device for all parameters and buffers in this module\n",
            " |          memory_format (:class:`torch.memory_format`): the desired memory\n",
            " |              format for 4D parameters and buffers in this module (keyword\n",
            " |              only argument)\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |      \n",
            " |      Examples::\n",
            " |      \n",
            " |          >>> linear = nn.Linear(2, 2)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]])\n",
            " |          >>> linear.to(torch.double)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1913, -0.3420],\n",
            " |                  [-0.5113, -0.2325]], dtype=torch.float64)\n",
            " |          >>> gpu1 = torch.device(\"cuda:1\")\n",
            " |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')\n",
            " |          >>> cpu = torch.device(\"cpu\")\n",
            " |          >>> linear.to(cpu)\n",
            " |          Linear(in_features=2, out_features=2, bias=True)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.1914, -0.3420],\n",
            " |                  [-0.5112, -0.2324]], dtype=torch.float16)\n",
            " |      \n",
            " |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)\n",
            " |          >>> linear.weight\n",
            " |          Parameter containing:\n",
            " |          tensor([[ 0.3741+0.j,  0.2382+0.j],\n",
            " |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)\n",
            " |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))\n",
            " |          tensor([[0.6122+0.j, 0.1150+0.j],\n",
            " |                  [0.6122+0.j, 0.1150+0.j],\n",
            " |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)\n",
            " |  \n",
            " |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T\n",
            " |      Moves the parameters and buffers to the specified device without copying storage.\n",
            " |      \n",
            " |      Args:\n",
            " |          device (:class:`torch.device`): The desired device of the parameters\n",
            " |              and buffers in this module.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  train(self: ~T, mode: bool = True) -> ~T\n",
            " |      Sets the module in training mode.\n",
            " |      \n",
            " |      This has any effect only on certain modules. See documentations of\n",
            " |      particular modules for details of their behaviors in training/evaluation\n",
            " |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,\n",
            " |      etc.\n",
            " |      \n",
            " |      Args:\n",
            " |          mode (bool): whether to set training mode (``True``) or evaluation\n",
            " |                       mode (``False``). Default: ``True``.\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  type(self: ~T, dst_type: Union[torch.dtype, str]) -> ~T\n",
            " |      Casts all parameters and buffers to :attr:`dst_type`.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Args:\n",
            " |          dst_type (type or string): the desired type\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T\n",
            " |      Moves all model parameters and buffers to the XPU.\n",
            " |      \n",
            " |      This also makes associated parameters and buffers different objects. So\n",
            " |      it should be called before constructing optimizer if the module will\n",
            " |      live on XPU while being optimized.\n",
            " |      \n",
            " |      .. note::\n",
            " |          This method modifies the module in-place.\n",
            " |      \n",
            " |      Arguments:\n",
            " |          device (int, optional): if specified, all parameters will be\n",
            " |              copied to that device\n",
            " |      \n",
            " |      Returns:\n",
            " |          Module: self\n",
            " |  \n",
            " |  zero_grad(self, set_to_none: bool = False) -> None\n",
            " |      Sets gradients of all model parameters to zero. See similar function\n",
            " |      under :class:`torch.optim.Optimizer` for more context.\n",
            " |      \n",
            " |      Args:\n",
            " |          set_to_none (bool): instead of setting to zero, set the grads to None.\n",
            " |              See :meth:`torch.optim.Optimizer.zero_grad` for details.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from torch.nn.modules.module.Module:\n",
            " |  \n",
            " |  T_destination = ~T_destination\n",
            " |  \n",
            " |  __annotations__ = {'__call__': typing.Callable[..., typing.Any], '_is_...\n",
            " |  \n",
            " |  dump_patches = False\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6Cp7Qbw6xS1"
      },
      "source": [
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OKrep6I7JqF",
        "outputId": "62c5f15c-64c0-4f81-80b4-8278f5544d36"
      },
      "source": [
        "vocab(['here', 'is', 'an', 'example', 'of', 'alien', 'invasion', 'they', 'are', 'called', 'bangalorites'])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1538, 6, 19, 1840, 4, 5328, 4138, 168, 47, 304, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQVgPe4P7Xef",
        "outputId": "f9fa46d5-5dc2-46f8-b4a8-0f32e1cf5225"
      },
      "source": [
        "vocab[\"<unk>\"]"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK1sxfsM7doh"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iiJdwCO79Yx",
        "outputId": "5914105b-f420-41fa-e194-c03f1fdfea18"
      },
      "source": [
        "text_pipeline(\"Here is an exmaple of alient invasion, and they are called bangaloriters\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1538, 6, 19, 0, 4, 0, 4138, 90515, 7, 168, 47, 304, 0]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TQusm2u8COn",
        "outputId": "18937313-5b6f-4c2b-a3fb-443609828db9"
      },
      "source": [
        "label_pipeline('19')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xArz2o1P8MKG"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def collate_fn(batch):\n",
        "  src_batch, tgt_batch = [], []\n",
        "  for src_batch, tgt_batch in batch:\n",
        "    src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip('\\n')))\n",
        "    tgt_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip('\\n')))\n",
        "  src_batch = pad_sequences(src_batch, padding_value=PAD_IDX)\n",
        "  tgt_batch = pad_sequences(tgt_batch, padding_value=PAD_IDX)\n",
        "  return src_batch, tgt_batch"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NFbwLqoN9K9p",
        "outputId": "6c659fe2-2845-438b-b4e7-9940e59419bb"
      },
      "source": [
        "weight = torch.randn(10, 5)\n",
        "weight"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.5944, -0.8532, -0.1321, -0.0680,  0.1143],\n",
              "        [-0.2533,  0.0119,  0.1340,  1.5424,  0.4331],\n",
              "        [-2.2236,  2.2732,  1.5229,  0.1983, -0.4779],\n",
              "        [-0.4710,  0.4432, -1.2667, -2.2129,  0.2228],\n",
              "        [ 1.5305, -1.3368,  0.8037, -0.5994,  0.4910],\n",
              "        [-0.5367,  1.5748,  0.2565,  0.8992, -0.4226],\n",
              "        [ 1.1191,  0.7422,  0.3512,  0.8502,  0.2880],\n",
              "        [ 0.1959, -0.8018, -1.4487, -0.4668, -0.4109],\n",
              "        [ 1.3160, -0.1775, -0.1438,  0.4066, -1.7205],\n",
              "        [-0.3187, -0.7105,  0.8181,  0.4216, -1.1274]])"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCobVaQ49umS",
        "outputId": "f9c78e6b-b9b1-47e1-db29-6e800fc66dbf"
      },
      "source": [
        "indices = torch.tensor([4, 1, 7])\n",
        "indices"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([4, 1, 7])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0texNYh90c6",
        "outputId": "aa763a27-5cf1-4795-f9d0-eb07f59a589c"
      },
      "source": [
        "embeddings = torch.nn.functional.embedding(indices, weight)\n",
        "embeddings"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.5305, -1.3368,  0.8037, -0.5994,  0.4910],\n",
              "        [-0.2533,  0.0119,  0.1340,  1.5424,  0.4331],\n",
              "        [ 0.1959, -0.8018, -1.4487, -0.4668, -0.4109]])"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1z-FVN19_VW",
        "outputId": "835deeb0-92ef-482e-9f6c-67aaa1e1e131"
      },
      "source": [
        "embeddings.mean(dim=0, keepdim=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4910, -0.7089, -0.1703,  0.1587,  0.1711]])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QrS2kb8p-i1Q",
        "outputId": "0cb0be72-4ce1-4aef-e142-70343a86d37e"
      },
      "source": [
        "torch.nn.functional.embedding_bag(indices, weight, torch.tensor([0]), mode=\"mean\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4910, -0.7089, -0.1703,  0.1587,  0.1711]])"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JSeLl_Sh-6ON"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)    "
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRDpOkXiAqyo"
      },
      "source": [
        "train_iter = DBpedia(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-IOBmF7A1N-"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKuQGv-xBIC1"
      },
      "source": [
        "1: World, 2: Sports, 3 Business, 4 Sci/Tech"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cW1rS0OYBGhR"
      },
      "source": [
        "train_iter = DBpedia(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtVCypgvBQfM"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1) # disuccees\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eV7lPedpBfpS",
        "outputId": "33403036-dcc3-4b92-c546-bd16495280ce"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = DBpedia()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| epoch   1 |   500/ 8313 batches | accuracy    0.697\n",
            "| epoch   1 |  1000/ 8313 batches | accuracy    0.909\n",
            "| epoch   1 |  1500/ 8313 batches | accuracy    0.938\n",
            "| epoch   1 |  2000/ 8313 batches | accuracy    0.954\n",
            "| epoch   1 |  2500/ 8313 batches | accuracy    0.958\n",
            "| epoch   1 |  3000/ 8313 batches | accuracy    0.960\n",
            "| epoch   1 |  3500/ 8313 batches | accuracy    0.965\n",
            "| epoch   1 |  4000/ 8313 batches | accuracy    0.968\n",
            "| epoch   1 |  4500/ 8313 batches | accuracy    0.967\n",
            "| epoch   1 |  5000/ 8313 batches | accuracy    0.970\n",
            "| epoch   1 |  5500/ 8313 batches | accuracy    0.969\n",
            "| epoch   1 |  6000/ 8313 batches | accuracy    0.971\n",
            "| epoch   1 |  6500/ 8313 batches | accuracy    0.974\n",
            "| epoch   1 |  7000/ 8313 batches | accuracy    0.973\n",
            "| epoch   1 |  7500/ 8313 batches | accuracy    0.973\n",
            "| epoch   1 |  8000/ 8313 batches | accuracy    0.973\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 52.93s | valid accuracy    0.975 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 8313 batches | accuracy    0.979\n",
            "| epoch   2 |  1000/ 8313 batches | accuracy    0.979\n",
            "| epoch   2 |  1500/ 8313 batches | accuracy    0.980\n",
            "| epoch   2 |  2000/ 8313 batches | accuracy    0.980\n",
            "| epoch   2 |  2500/ 8313 batches | accuracy    0.980\n",
            "| epoch   2 |  3000/ 8313 batches | accuracy    0.981\n",
            "| epoch   2 |  3500/ 8313 batches | accuracy    0.980\n",
            "| epoch   2 |  4000/ 8313 batches | accuracy    0.982\n",
            "| epoch   2 |  4500/ 8313 batches | accuracy    0.983\n",
            "| epoch   2 |  5000/ 8313 batches | accuracy    0.979\n",
            "| epoch   2 |  5500/ 8313 batches | accuracy    0.980\n",
            "| epoch   2 |  6000/ 8313 batches | accuracy    0.981\n",
            "| epoch   2 |  6500/ 8313 batches | accuracy    0.982\n",
            "| epoch   2 |  7000/ 8313 batches | accuracy    0.982\n",
            "| epoch   2 |  7500/ 8313 batches | accuracy    0.981\n",
            "| epoch   2 |  8000/ 8313 batches | accuracy    0.979\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 53.86s | valid accuracy    0.979 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  1000/ 8313 batches | accuracy    0.986\n",
            "| epoch   3 |  1500/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  2000/ 8313 batches | accuracy    0.986\n",
            "| epoch   3 |  2500/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  3000/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  3500/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  4000/ 8313 batches | accuracy    0.984\n",
            "| epoch   3 |  4500/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  5000/ 8313 batches | accuracy    0.986\n",
            "| epoch   3 |  5500/ 8313 batches | accuracy    0.986\n",
            "| epoch   3 |  6000/ 8313 batches | accuracy    0.986\n",
            "| epoch   3 |  6500/ 8313 batches | accuracy    0.986\n",
            "| epoch   3 |  7000/ 8313 batches | accuracy    0.986\n",
            "| epoch   3 |  7500/ 8313 batches | accuracy    0.985\n",
            "| epoch   3 |  8000/ 8313 batches | accuracy    0.985\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 54.72s | valid accuracy    0.979 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 8313 batches | accuracy    0.990\n",
            "| epoch   4 |  1000/ 8313 batches | accuracy    0.990\n",
            "| epoch   4 |  1500/ 8313 batches | accuracy    0.989\n",
            "| epoch   4 |  2000/ 8313 batches | accuracy    0.989\n",
            "| epoch   4 |  2500/ 8313 batches | accuracy    0.989\n",
            "| epoch   4 |  3000/ 8313 batches | accuracy    0.991\n",
            "| epoch   4 |  3500/ 8313 batches | accuracy    0.990\n",
            "| epoch   4 |  4000/ 8313 batches | accuracy    0.990\n",
            "| epoch   4 |  4500/ 8313 batches | accuracy    0.990\n",
            "| epoch   4 |  5000/ 8313 batches | accuracy    0.989\n",
            "| epoch   4 |  5500/ 8313 batches | accuracy    0.990\n",
            "| epoch   4 |  6000/ 8313 batches | accuracy    0.989\n",
            "| epoch   4 |  6500/ 8313 batches | accuracy    0.991\n",
            "| epoch   4 |  7000/ 8313 batches | accuracy    0.990\n",
            "| epoch   4 |  7500/ 8313 batches | accuracy    0.990\n",
            "| epoch   4 |  8000/ 8313 batches | accuracy    0.989\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 55.17s | valid accuracy    0.981 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  1000/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  1500/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  2000/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  2500/ 8313 batches | accuracy    0.989\n",
            "| epoch   5 |  3000/ 8313 batches | accuracy    0.991\n",
            "| epoch   5 |  3500/ 8313 batches | accuracy    0.989\n",
            "| epoch   5 |  4000/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  4500/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  5000/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  5500/ 8313 batches | accuracy    0.991\n",
            "| epoch   5 |  6000/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  6500/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  7000/ 8313 batches | accuracy    0.991\n",
            "| epoch   5 |  7500/ 8313 batches | accuracy    0.990\n",
            "| epoch   5 |  8000/ 8313 batches | accuracy    0.990\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 54.54s | valid accuracy    0.981 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 8313 batches | accuracy    0.990\n",
            "| epoch   6 |  1000/ 8313 batches | accuracy    0.990\n",
            "| epoch   6 |  1500/ 8313 batches | accuracy    0.990\n",
            "| epoch   6 |  2000/ 8313 batches | accuracy    0.991\n",
            "| epoch   6 |  2500/ 8313 batches | accuracy    0.989\n",
            "| epoch   6 |  3000/ 8313 batches | accuracy    0.990\n",
            "| epoch   6 |  3500/ 8313 batches | accuracy    0.991\n",
            "| epoch   6 |  4000/ 8313 batches | accuracy    0.991\n",
            "| epoch   6 |  4500/ 8313 batches | accuracy    0.991\n",
            "| epoch   6 |  5000/ 8313 batches | accuracy    0.990\n",
            "| epoch   6 |  5500/ 8313 batches | accuracy    0.990\n",
            "| epoch   6 |  6000/ 8313 batches | accuracy    0.991\n",
            "| epoch   6 |  6500/ 8313 batches | accuracy    0.991\n",
            "| epoch   6 |  7000/ 8313 batches | accuracy    0.990\n",
            "| epoch   6 |  7500/ 8313 batches | accuracy    0.990\n",
            "| epoch   6 |  8000/ 8313 batches | accuracy    0.991\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 54.01s | valid accuracy    0.981 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 8313 batches | accuracy    0.992\n",
            "| epoch   7 |  1000/ 8313 batches | accuracy    0.992\n",
            "| epoch   7 |  1500/ 8313 batches | accuracy    0.991\n",
            "| epoch   7 |  2000/ 8313 batches | accuracy    0.991\n",
            "| epoch   7 |  2500/ 8313 batches | accuracy    0.991\n",
            "| epoch   7 |  3000/ 8313 batches | accuracy    0.991\n",
            "| epoch   7 |  3500/ 8313 batches | accuracy    0.991\n",
            "| epoch   7 |  4000/ 8313 batches | accuracy    0.991\n",
            "| epoch   7 |  4500/ 8313 batches | accuracy    0.990\n",
            "| epoch   7 |  5000/ 8313 batches | accuracy    0.991\n",
            "| epoch   7 |  5500/ 8313 batches | accuracy    0.990\n",
            "| epoch   7 |  6000/ 8313 batches | accuracy    0.991\n",
            "| epoch   7 |  6500/ 8313 batches | accuracy    0.991\n",
            "| epoch   7 |  7000/ 8313 batches | accuracy    0.990\n",
            "| epoch   7 |  7500/ 8313 batches | accuracy    0.991\n",
            "| epoch   7 |  8000/ 8313 batches | accuracy    0.991\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 53.21s | valid accuracy    0.982 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 8313 batches | accuracy    0.991\n",
            "| epoch   8 |  1000/ 8313 batches | accuracy    0.991\n",
            "| epoch   8 |  1500/ 8313 batches | accuracy    0.991\n",
            "| epoch   8 |  2000/ 8313 batches | accuracy    0.991\n",
            "| epoch   8 |  2500/ 8313 batches | accuracy    0.991\n",
            "| epoch   8 |  3000/ 8313 batches | accuracy    0.990\n",
            "| epoch   8 |  3500/ 8313 batches | accuracy    0.991\n",
            "| epoch   8 |  4000/ 8313 batches | accuracy    0.990\n",
            "| epoch   8 |  4500/ 8313 batches | accuracy    0.992\n",
            "| epoch   8 |  5000/ 8313 batches | accuracy    0.991\n",
            "| epoch   8 |  5500/ 8313 batches | accuracy    0.992\n",
            "| epoch   8 |  6000/ 8313 batches | accuracy    0.991\n",
            "| epoch   8 |  6500/ 8313 batches | accuracy    0.991\n",
            "| epoch   8 |  7000/ 8313 batches | accuracy    0.991\n",
            "| epoch   8 |  7500/ 8313 batches | accuracy    0.990\n",
            "| epoch   8 |  8000/ 8313 batches | accuracy    0.990\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 52.99s | valid accuracy    0.982 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 8313 batches | accuracy    0.991\n",
            "| epoch   9 |  1000/ 8313 batches | accuracy    0.990\n",
            "| epoch   9 |  1500/ 8313 batches | accuracy    0.990\n",
            "| epoch   9 |  2000/ 8313 batches | accuracy    0.991\n",
            "| epoch   9 |  2500/ 8313 batches | accuracy    0.991\n",
            "| epoch   9 |  3000/ 8313 batches | accuracy    0.992\n",
            "| epoch   9 |  3500/ 8313 batches | accuracy    0.990\n",
            "| epoch   9 |  4000/ 8313 batches | accuracy    0.991\n",
            "| epoch   9 |  4500/ 8313 batches | accuracy    0.990\n",
            "| epoch   9 |  5000/ 8313 batches | accuracy    0.992\n",
            "| epoch   9 |  5500/ 8313 batches | accuracy    0.991\n",
            "| epoch   9 |  6000/ 8313 batches | accuracy    0.991\n",
            "| epoch   9 |  6500/ 8313 batches | accuracy    0.991\n",
            "| epoch   9 |  7000/ 8313 batches | accuracy    0.991\n",
            "| epoch   9 |  7500/ 8313 batches | accuracy    0.991\n",
            "| epoch   9 |  8000/ 8313 batches | accuracy    0.991\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 54.61s | valid accuracy    0.981 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 8313 batches | accuracy    0.991\n",
            "| epoch  10 |  1000/ 8313 batches | accuracy    0.991\n",
            "| epoch  10 |  1500/ 8313 batches | accuracy    0.991\n",
            "| epoch  10 |  2000/ 8313 batches | accuracy    0.992\n",
            "| epoch  10 |  2500/ 8313 batches | accuracy    0.991\n",
            "| epoch  10 |  3000/ 8313 batches | accuracy    0.992\n",
            "| epoch  10 |  3500/ 8313 batches | accuracy    0.991\n",
            "| epoch  10 |  4000/ 8313 batches | accuracy    0.991\n",
            "| epoch  10 |  4500/ 8313 batches | accuracy    0.991\n",
            "| epoch  10 |  5000/ 8313 batches | accuracy    0.991\n",
            "| epoch  10 |  5500/ 8313 batches | accuracy    0.991\n",
            "| epoch  10 |  6000/ 8313 batches | accuracy    0.990\n",
            "| epoch  10 |  6500/ 8313 batches | accuracy    0.991\n",
            "| epoch  10 |  7000/ 8313 batches | accuracy    0.991\n",
            "| epoch  10 |  7500/ 8313 batches | accuracy    0.992\n",
            "| epoch  10 |  8000/ 8313 batches | accuracy    0.990\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 55.06s | valid accuracy    0.981 \n",
            "-----------------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Qx3mEqsP0F2"
      },
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.init()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUrnPMhVBlq9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05f9c4c9-75ac-4c27-ac39-84a7bd798bc5"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.981\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76PGhuKbB4BN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f11c8f84-06f3-491c-c1e6-0ef08171ca2f"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This is a Business news\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-bXp1P9B55k"
      },
      "source": [
        ""
      ],
      "execution_count": 44,
      "outputs": []
    }
  ]
}