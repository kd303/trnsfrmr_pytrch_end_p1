Refer to **[PyTorchAssignment.ipynb](/pytorch_intro/PyTorchAssginment1.ipynb)** for below

## Data representation & Data generation Strategy

In the solution the approach that is considered is extending the ```MNIST``` class from ```torchvision.datasets```, overriding the getitem method to generate a random number using ```random.randrange``` to generate a random number between 0 and 9. And it returns a image_data, label and random number as well as the sum generated by random number 
Refer to ```MyDataset``` class which extends the MNIST and used in DataLoader


## how the two inputs are combined

![neuralnet](/pytorch_intro/nn.jpg) 
Refer to above image which describe the the concatination happens after image is passed throught convolution layers and random input is converted to one-hot encoded vector.

## Loss function selection and Why


Following are different attempts made for choosing the Loss function:

Idea was to take the average loss or weighted loss since the problem clearly states the accuracy of predicting sum is not as important as digits. So weighted loss will should have minimal impact on overall learning (ideally not consider at all?! :) )

1. Cross Entropy for DigitRecognition & Random Sum - simple sum
2. Cross Entropy for DigitRecognition & Random Sum - Weighted Sum, tried with different values (70-30), (90-10) weightages for digits and sums, this does change/reduce loss greatly
3. Cross Entropy for Digit Recongnition & RMSE for Sum - with weighted average - does not work significatly different, may be it seems there is an error in RMSE function

_FootNote_:

Here was my first thinking - basically based on log loss function intution, however that is not correct, basically the cross entropy is average of log loss of each class and hence below logic is **incorrect**, but keeping since it is good learning
Even though we are summing in overall function, CrossEntropy was only kept as loss function. Here the Loss is calculated separatey

And in cases where 

  - ```predicted_image_digit >= sum``` *then* ```loss = cross_entropy(predictied_image_digit)```
  - ```predicted_image_digit <= sum``` *then* ```loss = cross_entropy(sum)```

an interesting discovery to old math problem ```log(a+b) != log(a) + log(b)```

See the detailed [explanation](https://cdsmithus.medium.com/the-logarithm-of-a-sum-69dd76199790)


## evaluating your results 

Evaluation functions determines the correct digits vs corrrect sums..




## Training Logs
Printing the model, print twice since once on CPU and other on GPU
```
CustomCNN(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=212, out_features=212, bias=True)
  (fc2): Linear(in_features=212, out_features=100, bias=True)
  (out): Linear(in_features=100, out_features=29, bias=True)
  (preprocess_rand1): Linear(in_features=10, out_features=20, bias=True)
  (one_hot_input): One_Hot(10)
)
CrossEntropyLoss()
CustomCNN(
  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))
  (conv2): Conv2d(6, 12, kernel_size=(5, 5), stride=(1, 1))
  (fc1): Linear(in_features=212, out_features=212, bias=True)
  (fc2): Linear(in_features=212, out_features=100, bias=True)
  (out): Linear(in_features=100, out_features=29, bias=True)
  (preprocess_rand1): Linear(in_features=10, out_features=20, bias=True)
  (one_hot_input): One_Hot(10)
)
```
Following are the training Logs... Loss is not reducing after trying various lr and batch sizes...

```
epoch 0 total_correct_digits: 5842 digit_pred_loss: 2.367400884628296 total_correct_sum: 629 sum_pred_loss: 2.9444398880004883 loss: 1190.3136944770813
epoch 1 total_correct_digits: 5842 digit_pred_loss: 2.367400884628296 total_correct_sum: 656 sum_pred_loss: 2.9444398880004883 loss: 1190.3136924505234
epoch 2 total_correct_digits: 5842 digit_pred_loss: 2.273650884628296 total_correct_sum: 633 sum_pred_loss: 2.9444398880004883 loss: 1190.2972878217697
epoch 3 total_correct_digits: 5842 digit_pred_loss: 2.398650884628296 total_correct_sum: 643 sum_pred_loss: 2.9444398880004883 loss: 1190.3191632032394
epoch 4 total_correct_digits: 5842 digit_pred_loss: 2.398650884628296 total_correct_sum: 649 sum_pred_loss: 2.9444398880004883 loss: 1190.3191620111465
epoch 5 total_correct_digits: 5842 digit_pred_loss: 2.273650884628296 total_correct_sum: 707 sum_pred_loss: 2.9444398880004883 loss: 1190.2972911596298
epoch 6 total_correct_digits: 5842 digit_pred_loss: 2.367400884628296 total_correct_sum: 680 sum_pred_loss: 2.9444398880004883 loss: 1190.31369638443
epoch 7 total_correct_digits: 5842 digit_pred_loss: 2.336150884628296 total_correct_sum: 650 sum_pred_loss: 2.9444398880004883 loss: 1190.3082275390625
epoch 8 total_correct_digits: 5842 digit_pred_loss: 2.398650884628296 total_correct_sum: 644 sum_pred_loss: 2.9444398880004883 loss: 1190.3191647529602
epoch 9 total_correct_digits: 5842 digit_pred_loss: 2.429900884628296 total_correct_sum: 635 sum_pred_loss: 2.9444398880004883 loss: 1190.3246309757233
epoch 10 total_correct_digits: 5842 digit_pred_loss: 2.336150884628296 total_correct_sum: 668 sum_pred_loss: 2.9444398880004883 loss: 1190.3082257509232
epoch 11 total_correct_digits: 5842 digit_pred_loss: 2.336150884628296 total_correct_sum: 645 sum_pred_loss: 2.9444398880004883 loss: 1190.3082253932953
epoch 12 total_correct_digits: 5842 digit_pred_loss: 2.398650884628296 total_correct_sum: 697 sum_pred_loss: 2.9444398880004883 loss: 1190.3191641569138
epoch 13 total_correct_digits: 5842 digit_pred_loss: 2.398650884628296 total_correct_sum: 716 sum_pred_loss: 2.9444398880004883 loss: 1190.3191641569138
epoch 14 total_correct_digits: 5842 digit_pred_loss: 2.304900884628296 total_correct_sum: 651 sum_pred_loss: 2.9444398880004883 loss: 1190.302759885788
epoch 15 total_correct_digits: 5842 digit_pred_loss: 2.336150884628296 total_correct_sum: 674 sum_pred_loss: 2.9444398880004883 loss: 1190.3082265853882
epoch 16 total_correct_digits: 5842 digit_pred_loss: 2.336150884628296 total_correct_sum: 653 sum_pred_loss: 2.9444398880004883 loss: 1190.3082290887833
epoch 17 total_correct_digits: 5842 digit_pred_loss: 2.336150884628296 total_correct_sum: 670 sum_pred_loss: 2.9444398880004883 loss: 1190.3082259893417
epoch 18 total_correct_digits: 5842 digit_pred_loss: 2.429900884628296 total_correct_sum: 670 sum_pred_loss: 2.9444398880004883 loss: 1190.3246340751648
epoch 19 total_correct_digits: 5842 digit_pred_loss: 2.304900884628296 total_correct_sum: 652 sum_pred_loss: 2.9444398880004883 loss: 1190.30275785923
```
